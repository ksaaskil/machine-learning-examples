{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Style Transfer with Keras\n",
    "This notebook is based on Chapter 8.3 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python) by F. Chollet. Original article \"A Neural Algorithm of Artistic Style\" by Leon Gatys et al. can be found in [arXiv](https://arxiv.org/abs/1508.06576)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural style transfer means applying the style of a reference image to a target image while preserving the content of the target image. Here \"style\" means textures, colors, and visual patterns in the image. The content is the higher-level structure of the image. The loss function is heuristically defined as\n",
    "```python\n",
    "loss = dist(style(reference) - style(generated)) + dist(content(original) - content(generated)),\n",
    "```\n",
    "where `reference` is the reference image (from which style is copied), `original` is the image where style is being applied, and `generated` is the generated image. Minimizing this loss means that\n",
    "```\n",
    "style(generated) $\\approx$ style(reference)\n",
    "```\n",
    "and \n",
    "```\n",
    "content(generated) $\\approx$ content(original)\n",
    "```\n",
    "as we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key observation made in the paper by L. Gatys et al. was that convolutional networks (convnets) present a way to mathematically define `style` and `content` functions. The activations of the different layers of a convnet provide a decomposition of the contents of an image over different scales. The content of the image, which is a global and abstract property, is captured by the representations in the higher (later) layers of the convnet. \n",
    "\n",
    "The style loss as defined by Gatys et al. uses multiple layers of a convnet, as we want to capture the appearance of the reference image at all spatial scales extracted by the convnet. Gatys et al. use the _Gram matrix_ of a layer's activations as style loss, i.e., the inner product of the feature maps of a given layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now move on to the implementation following the general process:\n",
    "1. Set up a network that computes the layer activations for the style-reference image, target image, and the generated image.\n",
    "2. Use the computed layer activations to compute the loss function.\n",
    "3. Set up gradient descent to minimize the loss.\n",
    "\n",
    "We'll use the VGG19 as the pretrained convnet. Images are resized to a shared height of 400px."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define imports and constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "IS_KAGGLE = False  # Small helper: if running in KAGGLE, data can be found in \"../input\" instead of \"./input\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(image_path_or_array, ax=None):\n",
    "    \"\"\"\n",
    "    Show image.\n",
    "    :param image_path_or_array: Path to image or numpy array representing the image\n",
    "    :param ax: Axis where to plot. If not given, new figure is created.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        plt.figure()\n",
    "        ax = plt.gca()\n",
    "    if isinstance(image_path_or_array, np.ndarray):\n",
    "        img = image_path_or_array\n",
    "    elif isinstance(image_path_or_array, str):\n",
    "        try:  # Some of the images are somehow corrupt\n",
    "            img = mpimg.imread(image_path_or_array) # Numpy array\n",
    "        except Exception as ex:\n",
    "            print(\"Caught exception reading the image, returning.\", ex)\n",
    "            return\n",
    "        ax.set_title(\"/\".join(image_path_or_array.split(\"/\")[-2:]))\n",
    "    else:\n",
    "        raise RuntimeError(\"Unknown image type {type}\".format(type=type(image_path_or_array)))\n",
    "    \n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the base folder for our images and the types of art available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_img_folder = \"./input/dataset/dataset_updated/training_set\"\n",
    "if IS_KAGGLE:\n",
    "    base_img_folder = os.path.join(\"..\", base_img_folder)\n",
    "art_types = os.listdir(base_img_folder)\n",
    "print(art_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at drawings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images_for_art(art_type=\"drawings\", how_many=10):\n",
    "    assert art_type in art_types\n",
    "    img_folder = os.path.join(base_img_folder, art_type)\n",
    "\n",
    "    img_files = [os.path.join(img_folder, filename) for filename in os.listdir(img_folder)]\n",
    "\n",
    "    imgs_per_row = 5\n",
    "    nrows = (how_many - 1) // imgs_per_row + 1\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=imgs_per_row)\n",
    "    fig.set_size_inches((20, nrows * 5))\n",
    "    axes = axes.ravel()\n",
    "    for filename, ax in zip(img_files[:how_many], axes):\n",
    "        show_img(filename, ax=ax)\n",
    "        \n",
    "show_images_for_art(art_type=\"drawings\", how_many=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engravings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images_for_art(art_type=\"engraving\", how_many=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paintings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images_for_art(art_type=\"painting\", how_many=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iconography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images_for_art(art_type=\"iconography\", how_many=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sculptures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images_for_art(art_type=\"sculpture\", how_many=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the reference image\n",
    "Paintings, drawings, engravings and icons have a very clear style we could try transfering to our photo. Let's use pick one of them as our style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# style_image = \"painting/0918.jpg\"\n",
    "style_image = \"drawings/i - 593.jpeg\"\n",
    "style_reference_image_path = os.path.join(base_img_folder, style_image)\n",
    "show_img(style_reference_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the target image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_image = \"drawings/i - 655.jpeg\"\n",
    "target_image = \"iconography/84 18.59.20.jpg\"\n",
    "target_image_path = os.path.join(base_img_folder, target_image)\n",
    "\n",
    "show_img(target_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the target image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "width, height = load_img(target_image_path).size\n",
    "img_height = 400\n",
    "img_width = int(width * img_height / height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define image preprocessing function suitable for VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.applications import vgg19\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(img_height, img_width))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = vgg19.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "print(\"Shape of preprocessed target image:\", preprocess_image(target_image_path).shape)\n",
    "print(\"Shape of preprocessed reference image:\", preprocess_image(style_reference_image_path).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define inverse transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deprocess_image(x):\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define input tensors and pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "target_image = K.constant(preprocess_image(target_image_path))\n",
    "style_reference_image = K.constant(preprocess_image(style_reference_image_path))\n",
    "combination_image = K.placeholder((1, img_height, img_width, 3))\n",
    "\n",
    "input_tensor = K.concatenate([target_image, style_reference_image, combination_image], axis=0)\n",
    "\n",
    "model = vgg19.VGG19(input_tensor=input_tensor,\n",
    "                   weights='imagenet',\n",
    "                   include_top=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define content loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(base, combination):\n",
    "    return K.sum(K.square(combination-base))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define style loss in terms of the Gram matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
    "    gram = K.dot(features, K.transpose(features))\n",
    "    return gram\n",
    "\n",
    "def style_loss(style, combination):\n",
    "    S = gram_matrix(style)\n",
    "    C = gram_matrix(combination)\n",
    "    channels = 3\n",
    "    size = img_height * img_width\n",
    "    return K.sum(K.square(S - C)) / (4. * (channels ** 2) * (size ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add regularization loss to encourage continuity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variation_loss(x):\n",
    "    a = K.square(\n",
    "        x[:, :img_height - 1, :img_width - 1, :] - \n",
    "        x[:, 1:, :img_width - 1, :])\n",
    "    b = K.square(\n",
    "        x[:, :img_height - 1, :img_width - 1, :] - \n",
    "        x[:, :img_height - 1, 1:, :])\n",
    "    return K.sum(K.pow(a + b, 1.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save layers in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
    "outputs_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the layers used for computing style and content losses as well as loss itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_layer = 'block5_conv2'\n",
    "style_layers = ['block1_conv1',\n",
    "                'block2_conv1',\n",
    "                'block3_conv1',\n",
    "                'block4_conv1',\n",
    "                'block5_conv1']\n",
    "total_variation_weight = 1e-4\n",
    "style_weight = 1.\n",
    "content_weight = 0.025\n",
    "\n",
    "loss = K.variable(0.)\n",
    "layer_features = outputs_dict[content_layer]\n",
    "target_image_features = layer_features[0, :, :, :]\n",
    "combination_features = layer_features[2, :, :, :]\n",
    "loss += content_weight * content_loss(target_image_features, combination_features)\n",
    "\n",
    "for layer_name in style_layers:\n",
    "    layer_features = outputs_dict[layer_name]\n",
    "    style_reference_features = layer_features[1, :, :, :]\n",
    "    combination_features = layer_features[2, :, :, :]\n",
    "    sl = style_loss(style_reference_features, combination_features)\n",
    "    loss += (style_weight / len(style_layers)) * sl\n",
    "    \n",
    "loss += total_variation_weight * total_variation_loss(combination_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Keras functions that compute gradients and losses for given input placeholder tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = K.gradients(loss, combination_image)[0]\n",
    "fetch_loss_and_grads = K.function([combination_image], [loss, grads])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a helper class that computes both the loss and gradient in one go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self):\n",
    "        self.loss_value = None\n",
    "        self.grad_values = None\n",
    "    def loss(self, x):\n",
    "        assert self.loss_value is None\n",
    "        x = x.reshape((1, img_height, img_width, 3))\n",
    "        outs = fetch_loss_and_grads([x])\n",
    "        loss_value = outs[0]\n",
    "        grad_values = outs[1].flatten().astype('float64')\n",
    "        self.loss_value = loss_value\n",
    "        self.grad_values = grad_values\n",
    "        return self.loss_value\n",
    "    def grads(self, x):\n",
    "        assert self.loss_value is not None\n",
    "        grad_values = np.copy(self.grad_values)\n",
    "        self.loss_value = None\n",
    "        self.grad_values = None\n",
    "        return grad_values\n",
    "    \n",
    "evaluator = Evaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform loss minimization using Scipy's L-BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "from scipy.misc import imsave\n",
    "import time\n",
    "\n",
    "result_folder = 'results'\n",
    "if not IS_KAGGLE and not os.path.exists(result_folder):\n",
    "    os.makedirs(result_folder)\n",
    "\n",
    "iterations = 5\n",
    "\n",
    "x = preprocess_image(target_image_path)\n",
    "x = x.flatten()\n",
    "\n",
    "for i in range(iterations):\n",
    "    print('Start of iteration: {}'.format(i))\n",
    "    start_time = time.time()\n",
    "    x, min_val, info = fmin_l_bfgs_b(evaluator.loss,\n",
    "                                    x,\n",
    "                                    fprime=evaluator.grads,\n",
    "                                    maxfun=20)\n",
    "    print('Current loss value:', min_val)\n",
    "    img = x.copy().reshape((img_height, img_width, 3))\n",
    "    img = deprocess_image(img)\n",
    "    if not IS_KAGGLE:\n",
    "        fname = os.path.join(result_folder, 'generated_at_iteration_%d.png' % i)\n",
    "        imsave(fname, img)\n",
    "        print('Image saved as', fname)\n",
    "    show_img(img)\n",
    "    plt.show()\n",
    "    end_time = time.time()\n",
    "    print('Iteration %d completes in %d s' % (i, end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
