{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based on the Chapter 9 of [Hands-on Machine Learning with Scikit-Learn and TensorFlow](http://shop.oreilly.com/product/0636920052289.do) by A. Geron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of a simple computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.Variable(3, name = 'x')\n",
    "y = tf.Variable(4, name = 'y')\n",
    "\n",
    "# Computation, not yet evaluated\n",
    "f = x*x*y + y + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize a TensorFlow session, initialize the variables, and run the computation for the expected result of 42\n",
    "TensorFlow session places the operations onto devices such as CPUs and GPUs runs them. Session needs to be closed to free the resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(x.initializer)\n",
    "sess.run(y.initializer)\n",
    "result = sess.run(f)\n",
    "print('Result %.1f' % result)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to do the same thing using `with`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set `sess` as the default session, obtained from `tf.get_default_session()`\n",
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval() # Equivalent to `tf.get_default_session().run(f)\n",
    "    print('Result %.1f' % result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can also create an initializer that initializes all variables when it is run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    result = f.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Jupyter, it's useful to create an `InteractiveSession` that is set as the default session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "init.run()\n",
    "result = f.eval()\n",
    "print('Result %.1f' % result)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing graphs\n",
    "Any created node is automatically added to the default graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = tf.Variable(1)\n",
    "assert x1.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to create multiple independent computation graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x2 = tf.Variable(2)\n",
    "    \n",
    "assert x2.graph is graph\n",
    "assert x2.graph is not tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the same code multiple times as duplicate nodes to the default graph. Graph can be reseted as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = tf.Variable(1, name = 'x')\n",
    "x1 = tf.Variable(1, name = 'x')\n",
    "\n",
    "tf.reset_default_graph()\n",
    "assert len(tf.get_default_graph().as_graph_def().node) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lifecycle of node values\n",
    "For each node added to the graph, TF automatically determines the set of nodes it depends on and evaluates these nodes first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5 \n",
    "z = x * 3\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(y.eval()) # Evaluates x and then y\n",
    "    print(z.eval()) # Evalutes x -> y -> z without re-using the previously computed values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-evaluation can be avoided by evaluating `y` and `z` in one graph run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    y_val, z_val = sess.run([y, z])\n",
    "    print(y_val)\n",
    "    print(z_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing tensors\n",
    "Inputs and outputs in TF operations are tensors, represented as NumPy `ndarray`s in the Python API. Previous examples only involved scalars. One can perform computations on arrays of any shape. The example below performs linear regression for the California housing data set having 8 input features and one target (price). The optimal parameters are found by solving the normal equation $\\Theta = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}\\mathbf{y}$, where $\\mathbf{X}$ is the matrix of input features (including also the \"bias\" column of ones), $\\mathbf{y}$ contains the target values, and $\\Theta$ minimizes $||\\mathbf{X}\\Theta - \\mathbf{y}||^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "n_samples, n_features = housing.data.shape # Number of samples and features\n",
    "\n",
    "assert n_features == 8\n",
    "\n",
    "# Add bias input feature `x_0 = 1` in the feature set\n",
    "housing_data_plus_bias = np.c_[np.ones((n_samples ,1)), housing.data]\n",
    "\n",
    "assert housing_data_plus_bias.shape[1] == n_features + 1\n",
    "\n",
    "X = tf.constant(housing_data_plus_bias, dtype = tf.float32, name = 'X')\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype = tf.float32, name = 'y')\n",
    "\n",
    "XT = tf.transpose(X)\n",
    "\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval() # Runs in GPU if one available\n",
    "    \n",
    "print('Theta ', theta_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real problems require gradient descent, which can also be performed \"manually\" with TensorFlow. Here the gradient for the cost function is evaluated as \n",
    "$$\n",
    "\\nabla_{\\Theta} L = \\frac{2}{N} \\mathbf{X}^T (\\mathbf{y}^{pred} - \\mathbf{y}),\n",
    "$$\n",
    "or \n",
    "$$\n",
    "\\partial L/\\partial \\theta_i = \\frac{2}{N} \\sum_j X_{ji} (y_j^{pred} - y_j).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Scaling required for GD methods\n",
    "X_scaler = StandardScaler()\n",
    "scaled_housing_data = X_scaler.fit_transform(housing.data)\n",
    "\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((n_samples ,1)), scaled_housing_data]\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype = tf.float32, name = 'X')\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype = tf.float32, name = 'y')\n",
    "theta = tf.Variable(tf.random_uniform([n_features + 1, 1], -1.0, 1.0), name = 'theta')\n",
    "\n",
    "y_pred = tf.matmul(X, theta, name = 'predictions')\n",
    "error = y_pred - y\n",
    "\n",
    "mse = tf.reduce_mean(tf.square(error), name = 'mse')\n",
    "\n",
    "gradients = 2 / n_samples * tf.matmul(tf.transpose(X), error)\n",
    "# gradients = tf.gradients(mse, [theta])[0] # See below\n",
    "\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print('Epoch %d, mse %.2f' % (epoch, mse.eval()))\n",
    "        sess.run(training_op)\n",
    "        \n",
    "    best_theta = theta.eval()\n",
    "    \n",
    "print('Theta ', best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we do not generally need to evaluate the gradients manually. TensorFlow can do it using [reverse-mode autodiff](https://en.wikipedia.org/wiki/Automatic_differentiation#Reverse_accumulation) as:\n",
    "```python\n",
    "gradients = tf.gradients(mse, [theta])[0]\n",
    "```\n",
    "Replacing the analytical expression for `gradients` in the cell above leaves the result unchanged.\n",
    "\n",
    "Similarly, one does not need to implement the gradient descent themselves as TensorFlow provides the optimizers. One can replace the `training_op` in the above cell with the following code (and also delete `gradients`):\n",
    "\n",
    "```python\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can also implement Mini-batch GD using special `placeholder` nodes. `placeholder` nodes do not perform any computation, they only output the data that they are told to output in runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = tf.placeholder(tf.float32, shape = (None, 3)) # First dimension of A can have any size\n",
    "B = A + 5\n",
    "with tf.Session() as sess:\n",
    "    B_val_1 = B.eval(feed_dict = { A: [[1, 2, 3]]})\n",
    "    B_val_2 = B.eval(feed_dict = { A: [[4, 5, 6], [7, 8, 9]]})\n",
    "print(B_val_1)\n",
    "print(B_val_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code implements Mini-batch GD using `placeholder` nodes. In addition, it demonstrates saving the model and writing logs for Tensorboard visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(n_samples / batch_size))\n",
    "\n",
    "n_epochs = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "models_dir = 'models/'\n",
    "\n",
    "def tb_logdir():   \n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = 'tf_logs'\n",
    "\n",
    "    return os.path.join(root_logdir, 'run-%s' % now)\n",
    "\n",
    "logdir = tb_logdir()\n",
    "print('Using %s for TensorBoard logs' % logdir)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = (None, n_features + 1), name = 'X')\n",
    "y = tf.placeholder(tf.float32, shape = (None, 1), name = 'y')\n",
    "\n",
    "theta = tf.Variable(tf.random_uniform([n_features + 1, 1], -1.0, 1.0), name = 'theta')\n",
    "\n",
    "y_pred = tf.matmul(X, theta, name = 'predictions')\n",
    "\n",
    "# Error and MSE defined with \"loss\" namescope to reduce clutter\n",
    "with tf.name_scope(\"loss\") as scope:\n",
    "    error = y_pred - y\n",
    "    mse = tf.reduce_mean(tf.square(error), name = 'mse')\n",
    "    \n",
    "assert error.op.name == 'loss/sub'\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "y_targets = housing.target.reshape(-1, 1)\n",
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    inds = range(batch_index * batch_size, min((batch_index + 1) * batch_size, n_samples))\n",
    "    X_batch = scaled_housing_data_plus_bias[inds, :]\n",
    "    y_batch = y_targets[inds, :]\n",
    "    return X_batch, y_batch\n",
    "\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            \n",
    "            feed_dict = { X: X_batch, y: y_batch }\n",
    "            \n",
    "            # Write summary logs for TensorBoard visualization\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict = feed_dict)\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            \n",
    "            sess.run(training_op, feed_dict = feed_dict)\n",
    "\n",
    "        if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "            print('Epoch %d/%d, mse %.3f' % (epoch + 1, n_epochs, \n",
    "                                             mse.eval(feed_dict = {X: scaled_housing_data_plus_bias, \n",
    "                                                                   y: y_targets })))\n",
    "        if epoch % 25 == 0:\n",
    "            save_path = saver.save(sess, os.path.join(models_dir, 'mini-batch-gd.ckpt'))\n",
    "            print('Saved model to %s' % save_path)\n",
    "        \n",
    "    best_theta = theta.eval()\n",
    "    save_path = saver.save(sess, os.path.join(models_dir, 'mini-batch-gd-final.ckpt'))\n",
    "\n",
    "file_writer.close()\n",
    "print('Theta', best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing variables\n",
    "The below example defines a graph that computes the sum of five ReLU nodes. Each ReLU uses the same variable `threshold`, which is initialized in the first call to `relu()` function. Latter calls then re-use the value initialized in the first run (`reuse = tf.AUTO_REUSE`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    threshold = tf.get_variable('threshold', shape = (), initializer=tf.constant_initializer(0.0))\n",
    "    w_shape = (int(X.get_shape()[1]), 1)\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name = 'weights')\n",
    "    b = tf.Variable(0.0, name = 'bias')\n",
    "    z = tf.add(tf.matmul(X, w), b, name = 'z')\n",
    "    return tf.maximum(z, threshold, name = 'max')\n",
    "\n",
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, shape = (None, n_features), name = 'X')\n",
    "relus = []\n",
    "for relu_index in range(5):\n",
    "    # Define variable scope \"relu\", allowing for the re-use of variable \"threshold\" with `reuse = tf.AUTO_REUSE`.\n",
    "    # Setting this either `True` or `False` will raise an error because the variable \n",
    "    # is either not initialized in the first run or because it's initialized multiple times, respectively.\n",
    "    \n",
    "    # Note that each `with tf.variable_scope(...)` creates a new namescope for nodes such as `tf.Variable()`.\n",
    "    # Namescopes avoid name clashes by using running enumeration such as `relu_1`, `relu_2`, etc.\n",
    "    with tf.variable_scope('relu', reuse = tf.AUTO_REUSE) as scope:\n",
    "        relus.append(relu(X))\n",
    "output = tf.add_n(relus, name = 'output')\n",
    "\n",
    "file_writer = tf.summary.FileWriter(tb_logdir(), tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
