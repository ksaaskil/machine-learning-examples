{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training deep neural nets\n",
    "Adapted from Chap. 11 of `Hands-on Machine Learning with Scikit-Learn and TensorFlow` by A. Geron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main problems in training deep neural networks with millions of parametetrs\n",
    "1. Vanishing (or exploding) gradients that make lower layers hard to train\n",
    "2. Training is very slow\n",
    "3. Severe overfitting possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing gradients problem\n",
    "Backpropagation algorithm works by going from the output layer to the input layer, propagating the error gradient along the way. Gradients often get smaller and smaller as the algorithm progresses down to lower (closer to input) layers. Therefore, gradient descent (GD) update leaves the connection weights in the lower layers virtually unchanged and optimization never converges. Neural networks may also suffer from the _exploding gradients_ problem. This is especially the case with recurrent neural networks.\n",
    "\n",
    "In 2010, Glorot and Bengio showed that with logistic activation function and a simple mean-zero-std-one initialization, the variances of the outputs of each layer are much larger than the variances of the inputs. Therefore, the outputs in the top layers saturate close to zero or one and the gradients are very small. Therefore, the backpropagation algorithm has basically no gradient to propagate to the lower layers.\n",
    "\n",
    "As a solution, Glorot and Bengio proposed that the connection weights be initialized with zero-mean normal distribution with the following standard deviation $\\sigma$ or uniform distribution with the range $-r$ and $+r$:\n",
    "\n",
    "$$\n",
    "    \\sigma = \\sqrt{\\frac{2}{n_{inputs} + n_{outputs}}}, \\\\\n",
    "    r = \\sqrt{\\frac{6}{n_{inputs} + n_{outputs}}}\n",
    "$$\n",
    "Here $n_{inputs}$ and $n_{outputs}$ are the number of input and output connections for each layer. By default, `tf.layers.dense()` uses this initialization with a uniform distribution. One can use the similar _He initialization_ as follows:\n",
    "```python\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer(mode='FAN_AVG')\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, kernel_initializer=he_init, name=\"hidden1\")\n",
    "```\n",
    "\n",
    "### Nonsaturating activation functions\n",
    "As mentioned above, using the logistic activation function can lead to the vanishing gradients problem. For example, ReLU works well because it does not saturate for large positive values and it also fast to compute. ReLU suffers, however, partially from the same problem as the logistic activation function that the output values can saturate to zero. To solve this problem, one can use a variant of the ReLU such as `LeakyReLU`$(z)=\\max(\\alpha z, z)$, where typically $\\alpha=0.01$. The non-zero $alpha$ ensures that leaky ReLUs never completely die.\n",
    "\n",
    "Another variant is the exponential linear unit (ELU) defined as\n",
    "\n",
    "$$\n",
    "\\textrm{ELU}(z) = \\left\\{\\begin{array}{lr}\n",
    "        \\alpha(e^z - 1), & \\text{for } z < 0 \\\\\n",
    "        z, & \\text{for } z \\geq 0\n",
    "        \\end{array}\\right.\n",
    "$$\n",
    "\n",
    "Compared to ReLU, ELU has nonzero gradient for $z < 0$, is smooth everywhere, and has an average output closer to 0 at $x=0$. All these features mitigate the vanishing gradients problem. The only disadvantage with ELU is that it is slower to compute than ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch normalization\n",
    "He initialization and variants of ELU reduce the vanishing gradients problem at the beginning of training, but nothing guarantees it does not re-emerge during training. In 2015, Ioffe and Szegedy [proposed](https://arxiv.org/pdf/1502.03167v3.pdf) a technique called _Batch normalization_ (BN) to address the vanishing/exploding gradients problems, or more generally the problem that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change (Internal covariance shift problem). \n",
    "\n",
    "The technique consists of adding an operation in the model just before the activation function of each layer: zero-centering and normalizing the inputs, then scaling and shifting the result using two new parameters per layer (one for scaling, the other for shifting). This operationg lets the model learn the optimal scale and inputs for each layer.\n",
    "\n",
    "The algorithm estimates the inputs' mean and standard deviation by evaluating the mean and standard deviation of the inputs over the current mini-batch (hence the name Batch Normalization). In total, four parameters are learned for each batch-normalized layer: $\\gamma$ (scale of outputs), $\\beta$ (offset of outputs), $\\mu$ (mean of inputs), and $\\sigma$ (standard deviation of inputs).\n",
    "\n",
    "Ioffe and Szegedy showed that using batch normalization strongly reduced the vanishing gradients problem, reduced the sensitivity of training to the weight initialization, allowed for using much larger learning rates, and even acted as a regularization mitigating overfitting. Batch normalization adds complexity to the model and slows down training due to the extra computations required. Training can speed up once GD has found reasonably good values for the scales and offsets, though.\n",
    "\n",
    "See the example below for batch normalization in TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient clipping\n",
    "One technique to mitigate the exploding gradients problem is to [clip the gradients](http://proceedings.mlr.press/v28/pascanu13.pdf) during backpropagation so that they never exceed a given threshold. In TensorFlow, the optimizer's `minimize` function both computes the gradients and applies them to variables, so one must instead compute the gradients first, then create an operation to clip the gradients by value and finally apply the clipped gradients. The clip threshold is a hyperparameter that can be tuned. See the example below for gradient clipping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full example from Chap. 10 with He initialization, ELU activation function, batch normalization, and gradient clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ksaaskil/.pyenv/versions/3.5.0/envs/python3.5.0/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/mnist/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/mnist/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/mnist/data/t10k-labels-idx1-ubyte.gz\n",
      "Using mnist-logs/run-2018-09-08T10:20:08 for TensorBoard logs\n",
      "0 Train accuracy: 0.98 Val accuracy 0.951\n",
      "1 Train accuracy: 0.88 Val accuracy 0.963\n",
      "2 Train accuracy: 0.9 Val accuracy 0.9688\n",
      "3 Train accuracy: 0.88 Val accuracy 0.9736\n",
      "4 Train accuracy: 0.92 Val accuracy 0.9716\n",
      "5 Train accuracy: 0.92 Val accuracy 0.9756\n",
      "6 Train accuracy: 0.92 Val accuracy 0.9744\n",
      "7 Train accuracy: 0.92 Val accuracy 0.9754\n",
      "8 Train accuracy: 0.96 Val accuracy 0.9742\n",
      "9 Train accuracy: 0.96 Val accuracy 0.9766\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from functools import partial\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "n_inputs = 28*28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\") # Number of training samples in batch not known and not required\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") # y is a 1D array with unknown length\n",
    "\n",
    "# This will be set to True during training to tell batch normalization layers to use the whole training set's mean and stddev\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer(mode='FAN_AVG')\n",
    "\n",
    "# BN uses exponential decay with momentum to compute the running averages\n",
    "batch_norm_layer = partial(tf.layers.batch_normalization, training=training, momentum=0.9)\n",
    "\n",
    "# Neural network with batch-normalized layers\n",
    "with tf.name_scope(\"ann\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\", kernel_initializer=he_init)\n",
    "    bn1 = batch_norm_layer(hidden1)\n",
    "    bn1_act = tf.nn.elu(bn1)\n",
    "    hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\", kernel_initializer=he_init)\n",
    "    bn2 = batch_norm_layer(hidden2)\n",
    "    bn2_act = tf.nn.elu(bn2)\n",
    "    logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\", kernel_initializer=he_init) # Unscaled as softmax computed later internally\n",
    "    logits = batch_norm_layer(logits_before_bn, training=training, momentum=0.9)\n",
    "    \n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    # This op expects unscaled logits, since it performs a softmax on logits internally for efficiency.\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "learning_rate = 0.05\n",
    "gradient_clip_threshold = 1.0\n",
    "\n",
    "# Training with gradient clipping\n",
    "with tf.name_scope(\"train\"):\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    grads_and_vars = optimizer.compute_gradients(loss)\n",
    "    capped_cvs = [(tf.clip_by_value(grad, -gradient_clip_threshold, gradient_clip_threshold), var) \n",
    "                  for grad, var in grads_and_vars]\n",
    "    training_op = optimizer.apply_gradients(capped_cvs, global_step=global_step)\n",
    "\n",
    "# Evaluate performance by checking if the correct label is in top 1:\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
    "\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "summaries = tf.summary.merge_all()\n",
    "   \n",
    "saver = tf.train.Saver()\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/mnist/data\")\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 50\n",
    "\n",
    "# Batch normalization creates operations that must be evaluated at each step to update the moving averages\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "root_logdir = 'mnist-logs'\n",
    "\n",
    "def tb_logdir():   \n",
    "    now = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "    return os.path.join(root_logdir, 'run-%s' % now)\n",
    "\n",
    "logdir = tb_logdir()\n",
    "print('Using %s for TensorBoard logs' % logdir)\n",
    "\n",
    "SAVED_MODEL_PATH = os.path.join(logdir, 'model.ckpt')\n",
    "\n",
    "'''\n",
    "# This could be useful later\n",
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "            tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "'''\n",
    "        \n",
    "with tf.Session() as sess:\n",
    "    file_writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            summary, _, _ = sess.run([summaries, training_op, extra_update_ops], feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: mnist.validation.images, y: mnist.validation.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Val accuracy\", acc_val)\n",
    "        file_writer.add_summary(summary, epoch)\n",
    "    save_path = saver.save(sess, SAVED_MODEL_PATH)\n",
    "    file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning\n",
    "Transfer learning refers to re-using a model that was trained for a different task, typically using only the lower, more generic layers. Here's how one can import both the saved graph and its saved variables. For examples of transfer learning, see the autoencoder notebook where a classifier is added on top of a pretrained autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from mnist-logs/run-2018-09-08T10:20:08/model.ckpt\n",
      "Accuracy on test set: 0.9766\n"
     ]
    }
   ],
   "source": [
    "saved_model_meta = SAVED_MODEL_PATH + '.meta'\n",
    "saver = tf.train.import_meta_graph(SAVED_MODEL_PATH + '.meta')\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name('X:0')\n",
    "y = tf.get_default_graph().get_tensor_by_name('y:0')\n",
    "\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name('eval/accuracy/accuracy:0')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, SAVED_MODEL_PATH)\n",
    "    acc_test = accuracy.eval(feed_dict={X: mnist.validation.images, y: mnist.validation.labels})\n",
    "    \n",
    "print('Accuracy on test set:', acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8\n",
    "a. Build a neural network with five hidden layers of 100 neurons each, He initialization, and the ELU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "Extracting /tmp/mnist/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/mnist/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/mnist/data/t10k-labels-idx1-ubyte.gz\n",
      "Using 28038 training and 2558 validation images\n",
      "Using chap-11-exercise-8-logs/run-2018-09-08T10:20:37 for TensorBoard logs\n",
      "Epoch: 5 Training acc: 0.989 Validation acc: 0.98358095\n",
      "Epoch: 10 Training acc: 0.995 Validation acc: 0.9870993\n",
      "Epoch: 15 Training acc: 0.996 Validation acc: 0.9843628\n",
      "Epoch: 20 Training acc: 0.998 Validation acc: 0.9906177\n",
      "Epoch: 25 Training acc: 0.998 Validation acc: 0.98944485\n",
      "Epoch: 30 Training acc: 0.999 Validation acc: 0.98905396\n",
      "Epoch: 35 Training acc: 0.999 Validation acc: 0.9917905\n",
      "Epoch: 40 Training acc: 1.0 Validation acc: 0.9906177\n",
      "Epoch: 45 Training acc: 0.999 Validation acc: 0.9898358\n",
      "Epoch: 50 Training acc: 0.999 Validation acc: 0.9913995\n",
      "Epoch: 55 Training acc: 0.998 Validation acc: 0.9898358\n",
      "Epoch: 60 Training acc: 1.0 Validation acc: 0.9910086\n",
      "Epoch: 65 Training acc: 1.0 Validation acc: 0.9910086\n",
      "Epoch: 70 Training acc: 1.0 Validation acc: 0.9910086\n",
      "Epoch: 75 Training acc: 1.0 Validation acc: 0.9913995\n",
      "Epoch: 80 Training acc: 1.0 Validation acc: 0.9913995\n",
      "Epoch: 85 Training acc: 1.0 Validation acc: 0.9913995\n",
      "Epoch: 90 Training acc: 1.0 Validation acc: 0.9913995\n",
      "Epoch: 95 Training acc: 1.0 Validation acc: 0.9913995\n",
      "Epoch: 100 Training acc: 1.0 Validation acc: 0.9913995\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9913995"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "N_INPUTS = 28*28\n",
    "N_NEURONS = 100\n",
    "N_LAYERS = 5\n",
    "N_EPOCHS = 100\n",
    "BATCH_SIZE = 1000\n",
    "ROOT_LOGDIR = 'chap-11-exercise-8-logs'\n",
    "LEARNING_RATE = 0.01\n",
    "LABELS_INCLUDED = [0, 1, 2, 3, 4]\n",
    "N_CLASSES = len(LABELS_INCLUDED)\n",
    "\n",
    "def build_placeholders(n_inputs):\n",
    "    X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "    y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "    return X, y\n",
    "\n",
    "def build_model(X, \n",
    "                y, \n",
    "                n_neurons, \n",
    "                n_layers, \n",
    "                n_classes, \n",
    "                learning_rate, \n",
    "                kernel_reg=tf.contrib.layers.l2_regularizer(scale=0.0)):\n",
    "    \n",
    "    # kernel_reg = tf.contrib.layers.l2_regularizer(scale=0.05)\n",
    "    \n",
    "    def build_hidden_layers(X, n_layers, n_neurons, init):\n",
    "        neuron_layer = partial(tf.layers.dense, \n",
    "                               activation=tf.nn.elu, \n",
    "                               kernel_initializer=init, \n",
    "                               kernel_regularizer=kernel_reg)\n",
    "        top_layer = X\n",
    "        for ind in range(1, n_layers + 1):\n",
    "            layer_name = 'hidden-layer-' + str(ind)\n",
    "            top_layer = neuron_layer(top_layer, n_neurons, name=layer_name)\n",
    "        return top_layer      \n",
    "\n",
    "    '''\n",
    "    def build_hidden_layers(X, n_layers, n_neurons, init):\n",
    "        neuron_layer = partial(tf.layers.dense, activation=tf.nn.elu, kernel_initializer=init)\n",
    "        hidden1 = neuron_layer(X, n_neurons, name='hidden1')\n",
    "        hidden2 = neuron_layer(hidden1, n_neurons, name='hidden2')\n",
    "        hidden3 = neuron_layer(hidden2, n_neurons, name='hidden3')\n",
    "        hidden4 = neuron_layer(hidden3, n_neurons, name='hidden4')\n",
    "        hidden5 = neuron_layer(hidden4, n_neurons, name='hidden5')\n",
    "        return hidden5\n",
    "    '''\n",
    "\n",
    "    with tf.name_scope(\"hidden\"):\n",
    "        he_init = tf.variance_scaling_initializer()\n",
    "        top_hidden = build_hidden_layers(X, n_layers=n_layers, n_neurons=n_neurons, init=he_init)\n",
    "\n",
    "    \n",
    "    with tf.name_scope(\"logits\"):\n",
    "        logits = tf.layers.dense(top_hidden, n_classes, name=\"logits\")\n",
    "\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        # This op expects unscaled logits, since it performs a softmax on logits internally for efficiency.\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "        reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        base_loss = tf.reduce_mean(xentropy, name=\"base_loss\")\n",
    "        loss = tf.add_n([base_loss] + reg_losses, name=\"loss\")\n",
    "\n",
    "    with tf.name_scope(\"train\"):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "    \n",
    "    # Evaluate performance by checking if the correct label is in top 1:\n",
    "    with tf.name_scope(\"eval\"):\n",
    "        correct = tf.nn.in_top_k(logits, y, 1)\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
    "        \n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    summaries = tf.summary.merge_all()\n",
    "    return { 'accuracy': accuracy, 'summaries': summaries, 'training_op': training_op }\n",
    "\n",
    "\n",
    "def tb_logdir(root_logdir):   \n",
    "    now = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "    return os.path.join(root_logdir, 'run-%s' % now)\n",
    "\n",
    "\n",
    "def init_batch_iterator(mnist, X, y, labels_included, batch_size):\n",
    "    \n",
    "    def extract_data(all_images, all_labels):\n",
    "        inds_used = [ind for ind, label in enumerate(all_labels) if label in labels_included]\n",
    "        images = all_images[inds_used]\n",
    "        labels = all_labels[inds_used]\n",
    "        return images, labels\n",
    "    \n",
    "    train_images, train_labels = extract_data(mnist.train.images, mnist.train.labels)\n",
    "    val_images, val_labels = extract_data(mnist.validation.images, mnist.validation.labels)\n",
    "    \n",
    "    n_train = len(train_images)\n",
    "    assert n_train == len(train_labels)\n",
    "    \n",
    "    print('Using %d training and %d validation images' % (n_train, len(val_images)))\n",
    "    \n",
    "    def batch_iterator(train):\n",
    "        if train:\n",
    "            for i in range(n_train // batch_size):\n",
    "                inds = range(i*batch_size, (i+1)*batch_size)\n",
    "                yield { X: train_images[inds], y: train_labels[inds] }\n",
    "        else:\n",
    "            yield { X: val_images, y: val_labels }\n",
    "        return None\n",
    "\n",
    "    return batch_iterator\n",
    "\n",
    "MNIST = input_data.read_data_sets(\"/tmp/mnist/data\")\n",
    "\n",
    "def train(n_neurons, \n",
    "          n_layers, \n",
    "          n_epochs=N_EPOCHS, \n",
    "          verbose=True, \n",
    "          kernel_reg_scale=0.0):\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    X, y = build_placeholders(n_inputs=N_INPUTS)\n",
    "\n",
    "    model = build_model(X=X, \n",
    "                        y=y, \n",
    "                        n_neurons=n_neurons, \n",
    "                        n_layers=n_layers, \n",
    "                        n_classes=N_CLASSES,\n",
    "                        learning_rate=LEARNING_RATE)\n",
    "\n",
    "    accuracy, summaries, training_op = [model[key] for key in ['accuracy', 'summaries', 'training_op']]\n",
    "    \n",
    "    feed_batch_generator = init_batch_iterator(mnist=MNIST, \n",
    "                                               X=X, \n",
    "                                               y=y, \n",
    "                                               labels_included=LABELS_INCLUDED,\n",
    "                                               batch_size=BATCH_SIZE)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    logdir = tb_logdir(root_logdir=ROOT_LOGDIR)\n",
    "    if verbose:\n",
    "        print('Using %s for TensorBoard logs' % logdir)\n",
    "\n",
    "    saved_model_path = os.path.join(logdir, 'model_final.ckpt')\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        train_writer = tf.summary.FileWriter(logdir + '/train', sess.graph)\n",
    "        val_writer = tf.summary.FileWriter(logdir + '/val')\n",
    "        init.run()\n",
    "        \n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            for feed_dict in feed_batch_generator(train=True):\n",
    "                summary, train_acc, _ = sess.run([summaries, accuracy, training_op], feed_dict=feed_dict)\n",
    "\n",
    "            if epoch % 5 == 0:\n",
    "                val_generator = feed_batch_generator(train=False)\n",
    "                feed_dict = next(val_generator)\n",
    "                val_summary, val_acc = sess.run([summaries, accuracy], feed_dict=feed_dict)\n",
    "                train_writer.add_summary(summary, epoch)\n",
    "                val_writer.add_summary(val_summary, epoch)\n",
    "                assert next(val_generator, None) == None\n",
    "                saver.save(sess, os.path.join(logdir, 'model.ckpt'))\n",
    "                if verbose:\n",
    "                    print('Epoch:', epoch, 'Training acc:', train_acc, 'Validation acc:', val_acc)\n",
    "\n",
    "        saver.save(sess, os.path.join(logdir, 'model_final.ckpt'))\n",
    "        train_writer.close()\n",
    "        val_writer.close()\n",
    "    return val_acc\n",
    "        \n",
    "'''  \n",
    "regularization_grid = np.linspace(0, 0.05, 3)\n",
    "\n",
    "for kernel_reg in regularization_grid:\n",
    "    val_acc = train(n_neurons=N_NEURONS, \n",
    "                    n_layers=N_LAYERS, \n",
    "                    n_epochs=10, \n",
    "                    kernel_reg_scale=kernel_reg_scale)\n",
    "    print('')\n",
    "'''  \n",
    "train(n_neurons=N_NEURONS, n_layers=N_LAYERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us do the same as above but with `scikit-learn` compatible class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/mnist/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/mnist/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/mnist/data/t10k-labels-idx1-ubyte.gz\n",
      "5\tTraining loss: 0.181588\tValidation loss: 0.074442\tBest loss: 0.074442\tAccuracy: 97.81%\n",
      "10\tTraining loss: 0.150724\tValidation loss: 0.054044\tBest loss: 0.054044\tAccuracy: 98.40%\n",
      "15\tTraining loss: 0.136563\tValidation loss: 0.047813\tBest loss: 0.047813\tAccuracy: 98.55%\n",
      "20\tTraining loss: 0.121513\tValidation loss: 0.038035\tBest loss: 0.038035\tAccuracy: 98.75%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from datetime import datetime\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class NeuralNetworkClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 n_hidden_layers=5,\n",
    "                 n_neurons=100, \n",
    "                 learning_rate=0.01, \n",
    "                 batch_size=100,\n",
    "                 # n_epochs=10,\n",
    "                 activation=tf.nn.elu, \n",
    "                 initializer=tf.variance_scaling_initializer(), \n",
    "                 random_state=42,\n",
    "                 dropout_rate=None,\n",
    "                 batch_norm_momentum=None,\n",
    "                 root_logdir='chap-11-ex-8'):\n",
    "\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.n_neurons = n_neurons\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.activation = activation\n",
    "        self.initializer = initializer\n",
    "        # self.n_epochs = n_epochs\n",
    "        self.random_state = random_state\n",
    "        self.batch_norm_momentum = batch_norm_momentum\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self._session = None\n",
    "        self.root_logdir = root_logdir\n",
    "        \n",
    "    def _build_hidden(self, inputs):\n",
    "        for layer_index in range(self.n_hidden_layers):\n",
    "            with tf.name_scope(\"hidden\"):\n",
    "                if self.dropout_rate is not None:\n",
    "                    inputs = tf.layers.dropout(inputs, self.dropout_rate, name='hidden-drop-%d' % layer_index, training=self._training)\n",
    "                inputs = tf.layers.dense(inputs, \n",
    "                                         self.n_neurons, \n",
    "                                         name='hidden-%d' % (layer_index + 1), \n",
    "                                         kernel_initializer=self.initializer)\n",
    "                if self.batch_norm_momentum is not None:\n",
    "                    inputs = tf.layers.batch_normalization(inputs, \n",
    "                                                           training=self._training, \n",
    "                                                           momentum=self.batch_norm_momentum,\n",
    "                                                           name='hidden-%d-bn' % (layer_index + 1))\n",
    "                inputs = self.activation(inputs, name='hidden-%d-out' % (layer_index + 1))\n",
    "        return inputs\n",
    "    \n",
    "    def _build_graph(self, n_inputs, n_outputs):\n",
    "        if self.random_state is not None:\n",
    "            tf.set_random_seed(self.random_state)\n",
    "            np.random.seed(self.random_state)\n",
    "        X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "        y = tf.placeholder(tf.int32, shape=(None), name='y')\n",
    "        \n",
    "        if self.batch_norm_momentum or self.dropout_rate:\n",
    "            self._training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "        else:\n",
    "            self._training = None\n",
    "            \n",
    "        hidden_outputs = self._build_hidden(X)\n",
    "        \n",
    "        with tf.name_scope(\"logits\"):\n",
    "            logits = tf.layers.dense(hidden_outputs, n_outputs, kernel_initializer=self.initializer, name='logits')\n",
    "            y_proba = tf.nn.softmax(logits, name='Y_proba')\n",
    "        \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "            loss = tf.reduce_mean(xentropy, name='loss')\n",
    "        \n",
    "        with tf.name_scope(\"train\"):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "            training_op = optimizer.minimize(loss, name='training_op')\n",
    "        \n",
    "        with tf.name_scope(\"eval\"):\n",
    "            correct = tf.nn.in_top_k(logits, y, 1)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
    "            \n",
    "        with tf.name_scope(\"summary\"):\n",
    "            loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "            acc_summary = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "            \n",
    "        summary = tf.summary.merge_all()\n",
    "            \n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "        self._X = X\n",
    "        self._y = y\n",
    "        self._y_proba = y_proba\n",
    "        self._loss = loss\n",
    "        self._accuracy = accuracy\n",
    "        self._training_op = training_op\n",
    "        self._init = init\n",
    "        self._saver = saver\n",
    "        self._summary = summary\n",
    "        \n",
    "    def close_session(self):\n",
    "        if self._session:\n",
    "            self._session.close()\n",
    "    \n",
    "    @property\n",
    "    def _logdir(self): \n",
    "        now = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "        return os.path.join(self.root_logdir, 'run-%s' % now)\n",
    "\n",
    "\n",
    "    def fit(self, X, y, n_epochs=20, X_valid=None, y_valid=None):\n",
    "        # print('Shape of X:', X.shape)\n",
    "        # print('Shape of y:', y.shape)\n",
    "        self.close_session()\n",
    "        n_inputs = X.shape[1]\n",
    "        self.classes_ = np.unique(y)\n",
    "        # print(self.classes_)\n",
    "        n_outputs = len(self.classes_)\n",
    "        self.class_to_index_ = {label: index\n",
    "                            for index, label in enumerate(self.classes_)}\n",
    "        y = np.array([self.class_to_index_[label]\n",
    "                      for label in y], dtype=np.int32)\n",
    "        self._graph = tf.Graph()\n",
    "        with self._graph.as_default():\n",
    "            self._build_graph(n_inputs, n_outputs)\n",
    "            # extra ops for batch normalization\n",
    "            extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "        self._session = tf.Session(graph=self._graph)\n",
    "        best_loss = np.infty\n",
    "        \n",
    "        logdir = self._logdir\n",
    "        \n",
    "        with self._session.as_default() as sess:\n",
    "            train_writer = tf.summary.FileWriter(logdir + '/train', sess.graph)\n",
    "            val_writer = tf.summary.FileWriter(logdir + '/val')\n",
    "            self._init.run()\n",
    "            # self._saver.save()\n",
    "            for epoch in range(1, n_epochs + 1):\n",
    "                rnd_idx = np.random.permutation(len(X))\n",
    "                i_batch = 0\n",
    "                train_loss_sum = 0\n",
    "                for rnd_indices in np.array_split(rnd_idx, len(X) // self.batch_size):\n",
    "                    X_batch, y_batch = X[rnd_indices], y[rnd_indices]\n",
    "                    feed_dict = {self._X: X_batch, self._y: y_batch}\n",
    "                    if self._training is not None:\n",
    "                        feed_dict[self._training] = True\n",
    "                    train_loss, summary, _ = sess.run([self._loss, self._summary, self._training_op], feed_dict=feed_dict)\n",
    "                    train_loss_sum += train_loss\n",
    "                    i_batch += 1\n",
    "                    if extra_update_ops:\n",
    "                        sess.run(extra_update_ops, feed_dict=feed_dict)\n",
    "                \n",
    "                train_loss = train_loss_sum / i_batch\n",
    "                train_writer.add_summary(summary, epoch)\n",
    "                if epoch % 5 ==0 and X_valid is not None and y_valid is not None:\n",
    "                    loss_val, acc_val, summary = sess.run([self._loss, self._accuracy, self._summary],\n",
    "                                                 feed_dict={self._X: X_valid,\n",
    "                                                            self._y: y_valid})\n",
    "                    if loss_val < best_loss:\n",
    "                        best_loss = loss_val\n",
    "                    val_writer.add_summary(summary, epoch)\n",
    "                    print(\"{}\\tTraining loss: {:.6f}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "                        epoch, train_loss, loss_val, best_loss, acc_val * 100))\n",
    "            self._saver.save(sess, os.path.join(logdir, 'model_final.ckpt'))\n",
    "            train_writer.close()\n",
    "            val_writer.close()\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        if not self._session:\n",
    "            raise NotFittedError(\"This %s instance is not fitted yet\" % self.__class__.__name__)\n",
    "        with self._session.as_default() as sess:\n",
    "            return self._y_proba.eval(feed_dict={self._X: X})\n",
    "\n",
    "    def predict(self, X):\n",
    "        class_indices = np.argmax(self.predict_proba(X), axis=1)\n",
    "        return np.array([[self.classes_[class_index]]\n",
    "                         for class_index in class_indices], np.int32)\n",
    "\n",
    "    def save(self, path):\n",
    "        self._saver.save(self._session, path)\n",
    "\n",
    "N_INPUTS = 28*28\n",
    "N_OUTPUTS = 5\n",
    "\n",
    "MNIST = input_data.read_data_sets(\"/tmp/mnist/data\")\n",
    "X_train = MNIST.train.images\n",
    "y_train = MNIST.train.labels\n",
    "X_valid = MNIST.validation.images\n",
    "y_valid = MNIST.validation.labels\n",
    "\n",
    "inds = [ind for ind, label in enumerate(MNIST.train.labels) if label < 5]\n",
    "X_train1 = X_train[y_train < 5]\n",
    "y_train1 = y_train[y_train < 5]\n",
    "val_inds = [ind for ind, label in enumerate(MNIST.validation.labels) if label < 5]\n",
    "X_valid1 = X_valid[y_valid < 5]\n",
    "y_valid1 = y_valid[y_valid < 5]\n",
    "\n",
    "nn_clf = NeuralNetworkClassifier(batch_norm_momentum=0.9, \n",
    "                                 dropout_rate=0.5).fit(X_train1, \n",
    "                                              y_train1, \n",
    "                                              X_valid=X_valid1,\n",
    "                                              y_valid=y_valid1, \n",
    "                                              n_epochs=20)\n",
    "nn_clf.close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV] learning_rate=0.01, batch_size=50, n_neurons=100, batch_norm_momentum=None \n",
      "5\tTraining loss: 0.061119\tValidation loss: 0.065641\tBest loss: 0.065641\tAccuracy: 98.16%\n",
      "10\tTraining loss: 0.051024\tValidation loss: 0.065235\tBest loss: 0.065235\tAccuracy: 98.32%\n",
      "15\tTraining loss: 0.108033\tValidation loss: 0.112765\tBest loss: 0.065235\tAccuracy: 97.62%\n",
      "20\tTraining loss: 14.309467\tValidation loss: 2.163320\tBest loss: 0.065235\tAccuracy: 20.91%\n",
      "25\tTraining loss: 1.640989\tValidation loss: 2.071779\tBest loss: 0.065235\tAccuracy: 19.27%\n",
      "30\tTraining loss: 1.650063\tValidation loss: 2.042195\tBest loss: 0.065235\tAccuracy: 20.91%\n",
      "[CV]  learning_rate=0.01, batch_size=50, n_neurons=100, batch_norm_momentum=None, total=  18.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   18.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] learning_rate=0.01, batch_size=50, n_neurons=100, batch_norm_momentum=None \n",
      "5\tTraining loss: 0.064174\tValidation loss: 0.099718\tBest loss: 0.099718\tAccuracy: 97.85%\n",
      "10\tTraining loss: 0.075487\tValidation loss: 0.082349\tBest loss: 0.082349\tAccuracy: 97.89%\n",
      "15\tTraining loss: 0.175402\tValidation loss: 0.084672\tBest loss: 0.082349\tAccuracy: 98.16%\n",
      "20\tTraining loss: 0.025254\tValidation loss: 0.080152\tBest loss: 0.080152\tAccuracy: 98.63%\n",
      "25\tTraining loss: 0.313716\tValidation loss: 0.121536\tBest loss: 0.080152\tAccuracy: 97.38%\n",
      "30\tTraining loss: 0.039289\tValidation loss: 0.130459\tBest loss: 0.080152\tAccuracy: 97.30%\n",
      "[CV]  learning_rate=0.01, batch_size=50, n_neurons=100, batch_norm_momentum=None, total=  18.1s\n",
      "[CV] learning_rate=0.01, batch_size=50, n_neurons=100, batch_norm_momentum=None \n",
      "5\tTraining loss: 0.140219\tValidation loss: 0.111699\tBest loss: 0.111699\tAccuracy: 98.01%\n",
      "10\tTraining loss: 2.213844\tValidation loss: 0.522892\tBest loss: 0.111699\tAccuracy: 75.96%\n",
      "15\tTraining loss: 0.045604\tValidation loss: 0.102675\tBest loss: 0.102675\tAccuracy: 97.77%\n",
      "20\tTraining loss: 0.062521\tValidation loss: 0.164508\tBest loss: 0.102675\tAccuracy: 97.77%\n",
      "25\tTraining loss: 0.481170\tValidation loss: 0.258570\tBest loss: 0.102675\tAccuracy: 95.23%\n",
      "30\tTraining loss: 0.044649\tValidation loss: 0.151148\tBest loss: 0.102675\tAccuracy: 98.08%\n",
      "[CV]  learning_rate=0.01, batch_size=50, n_neurons=100, batch_norm_momentum=None, total=  18.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   55.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\tTraining loss: 0.205375\tValidation loss: 0.089270\tBest loss: 0.089270\tAccuracy: 97.69%\n",
      "10\tTraining loss: 0.055514\tValidation loss: 0.077511\tBest loss: 0.077511\tAccuracy: 98.36%\n",
      "15\tTraining loss: 0.249777\tValidation loss: 0.118785\tBest loss: 0.077511\tAccuracy: 98.24%\n",
      "20\tTraining loss: 0.058572\tValidation loss: 0.082875\tBest loss: 0.077511\tAccuracy: 98.48%\n",
      "25\tTraining loss: 0.069449\tValidation loss: 0.099119\tBest loss: 0.077511\tAccuracy: 97.97%\n",
      "30\tTraining loss: 1.808944\tValidation loss: 0.319903\tBest loss: 0.077511\tAccuracy: 95.35%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_neurons\": [100],\n",
    "    \"batch_size\": [50],\n",
    "    \"learning_rate\": [0.01],\n",
    "    \"batch_norm_momentum\": [None]\n",
    "    # \"activation\": [tf.nn.relu, tf.nn.elu, leaky_relu(alpha=0.01), leaky_relu(alpha=0.1)],\n",
    "    #\"n_hidden_layers\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    #\"optimizer_class\": [tf.train.AdamOptimizer, partial(tf.train.MomentumOptimizer, momentum=0.95)],\n",
    "}\n",
    "\n",
    "rnd_search = RandomizedSearchCV(NeuralNetworkClassifier(random_state=42), param_distribs, n_iter=1,\n",
    "                                random_state=42, verbose=2)\n",
    "cv = rnd_search.fit(X_train1, y_train1, X_valid=X_valid1, y_valid=y_valid1, n_epochs=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code below is for playing around with hyperparameter optimizers and organizing TF code, _not guaranteed to work_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of hyperparameter optimization with [hyperopt](https://hyperopt.github.io/hyperopt/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Installation from source required to get the latest version\n",
    "# !pip install --upgrade git+git://github.com/hyperopt/hyperopt.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing params: {'n_layers': 6.0, 'n_neurons': 60.0, 'n_epochs': 50}\n",
      "Using 28038 training and 2558 validation images\n",
      "Testing params: {'n_layers': 8.0, 'n_neurons': 20.0, 'n_epochs': 30}\n",
      "Using 28038 training and 2558 validation images\n",
      "Testing params: {'n_layers': 6.0, 'n_neurons': 100.0, 'n_epochs': 30}\n",
      "Using 28038 training and 2558 validation images\n",
      "Testing params: {'n_layers': 4.0, 'n_neurons': 90.0, 'n_epochs': 30}\n",
      "Using 28038 training and 2558 validation images\n",
      "Testing params: {'n_layers': 4.0, 'n_neurons': 40.0, 'n_epochs': 30}\n",
      "Using 28038 training and 2558 validation images\n",
      "Testing params: {'n_layers': 6.0, 'n_neurons': 50.0, 'n_epochs': 30}\n",
      "Using 28038 training and 2558 validation images\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a6e47ec2e721>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     trials=trials)\n\u001b[0m",
      "\u001b[0;32m/Users/ksaaskil/.pyenv/versions/3.5.0/envs/python3.5.0/lib/python3.5/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len)\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m         )\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ksaaskil/.pyenv/versions/3.5.0/envs/python3.5.0/lib/python3.5/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin)\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m             return_argmin=return_argmin)\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ksaaskil/.pyenv/versions/3.5.0/envs/python3.5.0/lib/python3.5/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len)\u001b[0m\n\u001b[1;32m    383\u001b[0m                     max_queue_len=max_queue_len)\n\u001b[1;32m    384\u001b[0m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ksaaskil/.pyenv/versions/3.5.0/envs/python3.5.0/lib/python3.5/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ksaaskil/.pyenv/versions/3.5.0/envs/python3.5.0/lib/python3.5/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstopped\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ksaaskil/.pyenv/versions/3.5.0/envs/python3.5.0/lib/python3.5/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'job exception: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ksaaskil/.pyenv/versions/3.5.0/envs/python3.5.0/lib/python3.5/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    838\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[0;32m--> 840\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-a6e47ec2e721>\u001b[0m in \u001b[0;36mf_nn\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     13\u001b[0m                     \u001b[0mn_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_layers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     verbose=False)\n\u001b[0m\u001b[1;32m     16\u001b[0m     return {\n\u001b[1;32m     17\u001b[0m             \u001b[0;34m'loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-73e55e0e2f59>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(n_neurons, n_layers, n_epochs, verbose, kernel_reg_scale)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeed_batch_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m                 \u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msummaries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-73e55e0e2f59>\u001b[0m in \u001b[0;36mbatch_iterator\u001b[0;34m(train)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_train\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0minds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minds\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval_labels\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "space = {\n",
    "            'n_neurons': hp.quniform('n_neurons', 10, 100, 10),\n",
    "            'n_layers': hp.quniform('n_layers', 1, 9, 2),\n",
    "            'n_epochs': hp.choice('n_epochs', [30, 50])\n",
    "        }\n",
    "\n",
    "def f_nn(params):\n",
    "    print('Testing params:', params)\n",
    "    val_acc = train(n_neurons=int(params['n_neurons']), \n",
    "                    n_layers=int(params['n_layers']),\n",
    "                    n_epochs=int(params['n_epochs']),\n",
    "                    verbose=False)\n",
    "    return {\n",
    "            'loss': -val_acc,\n",
    "            'val_acc': val_acc,\n",
    "            'params': params,\n",
    "            'status': STATUS_OK\n",
    "           }\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "best = fmin(f_nn,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=20,\n",
    "    trials=trials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Flatten params to columns of their own\n",
    "for trial in trials.results:\n",
    "    params = dict(trial['params'])\n",
    "    for key in params.keys():\n",
    "        trial[key] = params[key]\n",
    "\n",
    "pd.DataFrame.from_dict(trials.results).sort_values(by='loss', axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of hyperparameter optimization with [Tune](https://ray.readthedocs.io/en/latest/tune-usage.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "import ray.tune as tune\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_func(params, reporter):\n",
    "    print('Testing params:', params)\n",
    "    val_acc = train(n_neurons=params['n_neurons'], \n",
    "                    n_layers=params['n_layers'],\n",
    "                    n_epochs=params['n_epochs'],\n",
    "                    verbose=False)\n",
    "    reporter(val_acc=val_acc)\n",
    "    return None\n",
    "\n",
    "all_trials = tune.run_experiments({\n",
    "    \"my_experiment\": {\n",
    "        \"run\": train_func,\n",
    "        \"config\": {\"n_neurons\": tune.grid_search([20, 50, 100]),\n",
    "                    \"n_layers\": tune.grid_search([1, 5, 10]),\n",
    "                    \"n_epochs\": tune.grid_search([50])\n",
    "                  }\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skopt import gp_minimize\n",
    "\n",
    "def f(x):\n",
    "    return (np.sin(5 * x[0]) * (1 - np.tanh(x[0] ** 2)) *\n",
    "            np.random.randn() * 0.1)\n",
    "\n",
    "res = gp_minimize(f, [(-2.0, 2.0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some TensorFlow tips and tricks for easier organization of code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toy example of minimizing $(X + z + 1.0)^2$, where input $X=1.0$. Expected output is $z=2.0$. The example demonstrates using decorators to set default graph, name scopes, and variable scopes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from functools import wraps\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def with_return_graph(graph):\n",
    "    def inner_function(function):\n",
    "        @wraps(function)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # print(\"Arguments passed to decorator:\", graph)\n",
    "            with graph.as_default():\n",
    "                function(*args, **kwargs)\n",
    "            return graph\n",
    "        return wrapper\n",
    "    return inner_function\n",
    "\n",
    "def with_name_scope(scope_name):\n",
    "    def inner_func(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            with tf.name_scope(scope_name):\n",
    "                return func(*args, **kwargs)\n",
    "        return wrapper\n",
    "    return inner_func\n",
    "\n",
    "def with_variable_scope(scope_name, **kwargs_scope):\n",
    "    def inner_func(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            with tf.variable_scope(scope_name, **kwargs_scope):\n",
    "                return func(*args, **kwargs)\n",
    "        return wrapper\n",
    "    return inner_func\n",
    "\n",
    "# graph = tf.Graph() # Alternative way to fill graph with decorators\n",
    "# @with_return_graph(graph)\n",
    "def build_graph():\n",
    "    \n",
    "    @with_name_scope('input')\n",
    "    def add_inputs():\n",
    "        X = tf.placeholder(tf.float32, shape=(None), name=\"X\")\n",
    "        @with_variable_scope('variables', initializer=tf.random_normal_initializer(stddev=1.0))\n",
    "        def add_variables():\n",
    "            return tf.get_variable(name=\"z\", shape=())\n",
    "        z = add_variables()\n",
    "        w = tf.add(X, z, name='w')\n",
    "        return w\n",
    "    \n",
    "    @with_name_scope('prediction')\n",
    "    def add_prediction(X):\n",
    "        return tf.add(X, 1.0, name='y')\n",
    "    \n",
    "    @with_name_scope('loss')\n",
    "    def add_loss(y):\n",
    "        return tf.square(y, name='loss')\n",
    "    \n",
    "    @with_name_scope('summaries')\n",
    "    def add_summaries(loss):\n",
    "        loss_summary = tf.summary.scalar('loss', loss)\n",
    "        return tf.summary.merge_all()\n",
    "    \n",
    "    # @with_variable_scope(\"parameters\", reuse=True)\n",
    "    # def get_parameters():\n",
    "    #     return { 'learning_rate': tf.get_variable(\"learning_rate\") }\n",
    "    \n",
    "    def get_parameters():\n",
    "        graph = tf.get_default_graph()\n",
    "        return { 'learning_rate': graph.get_tensor_by_name(\"parameters/learning_rate:0\")}\n",
    "    \n",
    "    @with_name_scope('train')\n",
    "    def add_optimizer(loss, learning_rate):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        training_op = optimizer.minimize(loss, name='training_op')\n",
    "        return training_op\n",
    "    \n",
    "    w = add_inputs() # X + z\n",
    "    y = add_prediction(w) # X + z + 1.0\n",
    "    loss = add_loss(y) # y^2\n",
    "        \n",
    "    summary = add_summaries(loss)\n",
    "    params = get_parameters()\n",
    "    training_op = add_optimizer(loss, learning_rate=params['learning_rate'])\n",
    "    \n",
    "    return tf.get_default_graph()\n",
    "\n",
    "def tb_logdir(root_logdir):   \n",
    "    now = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "    return os.path.join(root_logdir, 'run-%s' % now)\n",
    "\n",
    "# graph = tf.Graph()\n",
    "\n",
    "# @with_return_graph(graph)\n",
    "# def fill_graph():\n",
    "#     return build_graph()\n",
    "\n",
    "N_EPOCHS = 50\n",
    "LEARNING_RATE = 0.1\n",
    "ROOT_LOGDIR = 'tf-logs/toy-example/'\n",
    "X_VAL = 1.0\n",
    "\n",
    "'''\n",
    "@with_variable_scope(\"parameters\", reuse=False)\n",
    "def set_constants(**kwargs):\n",
    "    for key in kwargs:\n",
    "        tf.get_variable(key, shape=(), initializer=tf.constant_initializer(kwargs[key]))\n",
    "'''\n",
    "\n",
    "@with_name_scope(\"parameters\")\n",
    "def set_constants(**kwargs):\n",
    "    for key in kwargs:\n",
    "        # tf.get_variable(key, shape=(), initializer=tf.constant_initializer(kwargs[key]))\n",
    "        tf.constant(kwargs[key], name=key)\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    set_constants(learning_rate=LEARNING_RATE)\n",
    "    graph = build_graph()\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "X = graph.get_tensor_by_name(\"input/X:0\") # Placeholder for input\n",
    "y = graph.get_tensor_by_name(\"prediction/y:0\") # X + z + 1.0\n",
    "z = graph.get_tensor_by_name(\"variables/z:0\") # Variable to minimize\n",
    "loss = graph.get_tensor_by_name(\"loss/loss:0\") # Loss (X + z + 1.0)^2\n",
    "training_op = graph.get_operation_by_name(\"train/training_op\") # Operation updating z\n",
    "learning_rate = graph.get_tensor_by_name(\"parameters/learning_rate:0\") # Accessing parameters\n",
    "summaries = graph.get_tensor_by_name(\"summaries/Merge/MergeSummary:0\") # Accessing merged summary\n",
    "\n",
    "logdir = tb_logdir(ROOT_LOGDIR)\n",
    "saved_model_path = os.path.join(logdir, 'model_final.ckpt')\n",
    "\n",
    "# print(graph.get_operations())\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    file_writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "    init.run()\n",
    "    feed_dict = { X: X_VAL }\n",
    "    z_val, y_val, loss_val = sess.run([z, y, loss], feed_dict=feed_dict)\n",
    "    \n",
    "    for epoch in range(1, N_EPOCHS+1):\n",
    "        loss_val, summary_val, _ = sess.run([loss, summaries, training_op], feed_dict=feed_dict)\n",
    "        if epoch % 5 == 0:\n",
    "            print('Epoch:', epoch, 'loss:', loss_val)\n",
    "            file_writer.add_summary(summary_val, epoch)\n",
    "    \n",
    "    z_val, y_val, loss_val = sess.run([z, y, loss], feed_dict=feed_dict)\n",
    "    saver.save(sess, saved_model_path)\n",
    "    file_writer.close()\n",
    "    \n",
    "print('Got value %.2f for z, expected -2.0' % (z_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
