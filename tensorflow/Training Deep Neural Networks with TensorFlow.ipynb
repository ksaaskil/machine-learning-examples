{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training deep neural nets\n",
    "Adapted from Chap. 11 of `Hands-on Machine Learning with Scikit-Learn and TensorFlow` by A. Geron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main problems in training deep neural networks with millions of parametetrs\n",
    "1. Vanishing (or exploding) gradients that make lower layers hard to train\n",
    "2. Training is very slow\n",
    "3. Severe overfitting possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing gradients problem\n",
    "Backpropagation algorithm works by going from the output layer to the input layer, propagating the error gradient along the way. Gradients often get smaller and smaller as the algorithm progresses down to lower (closer to input) layers. Therefore, gradient descent (GD) update leaves the connection weights in the lower layers virtually unchanged and optimization never converges. Neural networks may also suffer from the _exploding gradients_ problem. This is especially the case with recurrent neural networks.\n",
    "\n",
    "In 2010, Glorot and Bengio showed that with logistic activation function and a simple mean-zero-std-one initialization, the variances of the outputs of each layer are much larger than the variances of the inputs. Therefore, the outputs in the top layers saturate close to zero or one and the gradients are very small. Therefore, the backpropagation algorithm has basically no gradient to propagate to the lower layers.\n",
    "\n",
    "As a solution, Glorot and Bengio proposed that the connection weights be initialized with zero-mean normal distribution with the following standard deviation $\\sigma$ or uniform distribution with the range $-r$ and $+r$:\n",
    "\n",
    "$$\n",
    "    \\sigma = \\sqrt{\\frac{2}{n_{inputs} + n_{outputs}}}, \\\\\n",
    "    r = \\sqrt{\\frac{6}{n_{inputs} + n_{outputs}}}\n",
    "$$\n",
    "Here $n_{inputs}$ and $n_{outputs}$ are the number of input and output connections for each layer. By default, `tf.layers.dense()` uses this initialization with a uniform distribution. One can use the similar _He initialization_ as follows:\n",
    "```python\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer(mode='FAN_AVG')\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, kernel_initializer=he_init, name=\"hidden1\")\n",
    "```\n",
    "\n",
    "### Nonsaturating activation functions\n",
    "As mentioned above, using the logistic activation function can lead to the vanishing gradients problem. For example, ReLU works well because it does not saturate for large positive values and it also fast to compute. ReLU suffers, however, partially from the same problem as the logistic activation function that the output values can saturate to zero. To solve this problem, one can use a variant of the ReLU such as `LeakyReLU`$(z)=\\max(\\alpha z, z)$, where typically $\\alpha=0.01$. The non-zero $alpha$ ensures that leaky ReLUs never completely die.\n",
    "\n",
    "Another variant is the exponential linear unit (ELU) defined as\n",
    "\n",
    "$$\n",
    "\\textrm{ELU}(z) = \\left\\{\\begin{array}{lr}\n",
    "        \\alpha(e^z - 1), & \\text{for } z < 0 \\\\\n",
    "        z, & \\text{for } z \\geq 0\n",
    "        \\end{array}\\right.\n",
    "$$\n",
    "\n",
    "Compared to ReLU, ELU has nonzero gradient for $z < 0$, is smooth everywhere, and has an average output closer to 0 at $x=0$. All these features mitigate the vanishing gradients problem. The only disadvantage with ELU is that it is slower to compute than ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch normalization\n",
    "He initialization and variants of ELU reduce the vanishing gradients problem at the beginning of training, but nothing guarantees it does not re-emerge during training. In 2015, Ioffe and Szegedy [proposed](https://arxiv.org/pdf/1502.03167v3.pdf) a technique called _Batch normalization_ (BN) to address the vanishing/exploding gradients problems, or more generally the problem that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change (Internal covariance shift problem). \n",
    "\n",
    "The technique consists of adding an operation in the model just before the activation function of each layer: zero-centering and normalizing the inputs, then scaling and shifting the result using two new parameters per layer (one for scaling, the other for shifting). This operationg lets the model learn the optimal scale and inputs for each layer.\n",
    "\n",
    "The algorithm estimates the inputs' mean and standard deviation by evaluating the mean and standard deviation of the inputs over the current mini-batch (hence the name Batch Normalization). In total, four parameters are learned for each batch-normalized layer: $\\gamma$ (scale of outputs), $\\beta$ (offset of outputs), $\\mu$ (mean of inputs), and $\\sigma$ (standard deviation of inputs).\n",
    "\n",
    "Ioffe and Szegedy showed that using batch normalization strongly reduced the vanishing gradients problem, reduced the sensitivity of training to the weight initialization, allowed for using much larger learning rates, and even acted as a regularization mitigating overfitting. Batch normalization adds complexity to the model and slows down training due to the extra computations required. Training can speed up once GD has found reasonably good values for the scales and offsets, though.\n",
    "\n",
    "See the example below for batch normalization in TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient clipping\n",
    "One technique to mitigate the exploding gradients problem is to [clip the gradients](http://proceedings.mlr.press/v28/pascanu13.pdf) during backpropagation so that they never exceed a given threshold. In TensorFlow, the optimizer's `minimize` function both computes the gradients and applies them to variables, so one must instead compute the gradients first, then create an operation to clip the gradients by value and finally apply the clipped gradients. The clip threshold is a hyperparameter that can be tuned. See the example below for gradient clipping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full example from Chap. 10 with He initialization, ELU activation function, batch normalization, and gradient clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ksaaskil/.pyenv/versions/3.5.0/envs/python3.5.0/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/mnist/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/mnist/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/mnist/data/t10k-labels-idx1-ubyte.gz\n",
      "Using mnist-logs/run-2018-09-04T09:52:21 for TensorBoard logs\n",
      "0 Train accuracy: 0.96 Val accuracy 0.9466\n",
      "1 Train accuracy: 0.88 Val accuracy 0.9624\n",
      "2 Train accuracy: 0.92 Val accuracy 0.9704\n",
      "3 Train accuracy: 0.94 Val accuracy 0.9734\n",
      "4 Train accuracy: 0.9 Val accuracy 0.9726\n",
      "5 Train accuracy: 0.88 Val accuracy 0.9762\n",
      "6 Train accuracy: 0.82 Val accuracy 0.9774\n",
      "7 Train accuracy: 0.9 Val accuracy 0.9762\n",
      "8 Train accuracy: 0.9 Val accuracy 0.9768\n",
      "9 Train accuracy: 0.96 Val accuracy 0.9786\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from functools import partial\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "n_inputs = 28*28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\") # Number of training samples in batch not known and not required\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") # y is a 1D array with unknown length\n",
    "\n",
    "# This will be set to True during training to tell batch normalization layers to use the whole training set's mean and stddev\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer(mode='FAN_AVG')\n",
    "\n",
    "# BN uses exponential decay with momentum to compute the running averages\n",
    "batch_norm_layer = partial(tf.layers.batch_normalization, training=training, momentum=0.9)\n",
    "\n",
    "# Neural network with batch-normalized layers\n",
    "with tf.name_scope(\"ann\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\", kernel_initializer=he_init)\n",
    "    bn1 = batch_norm_layer(hidden1)\n",
    "    bn1_act = tf.nn.elu(bn1)\n",
    "    hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\", kernel_initializer=he_init)\n",
    "    bn2 = batch_norm_layer(hidden2)\n",
    "    bn2_act = tf.nn.elu(bn2)\n",
    "    logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\", kernel_initializer=he_init) # Unscaled as softmax computed later internally\n",
    "    logits = batch_norm_layer(logits_before_bn, training=training, momentum=0.9)\n",
    "    \n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    # This op expects unscaled logits, since it performs a softmax on logits internally for efficiency.\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "learning_rate = 0.05\n",
    "gradient_clip_threshold = 1.0\n",
    "\n",
    "# Training with gradient clipping\n",
    "with tf.name_scope(\"train\"):\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    grads_and_vars = optimizer.compute_gradients(loss)\n",
    "    capped_cvs = [(tf.clip_by_value(grad, -gradient_clip_threshold, gradient_clip_threshold), var) \n",
    "                  for grad, var in grads_and_vars]\n",
    "    training_op = optimizer.apply_gradients(capped_cvs, global_step=global_step)\n",
    "\n",
    "# Evaluate performance by checking if the correct label is in top 1:\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
    "\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "summaries = tf.summary.merge_all()\n",
    "   \n",
    "saver = tf.train.Saver()\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/mnist/data\")\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 50\n",
    "\n",
    "# Batch normalization creates operations that must be evaluated at each step to update the moving averages\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "root_logdir = 'mnist-logs'\n",
    "\n",
    "def tb_logdir():   \n",
    "    now = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "    return os.path.join(root_logdir, 'run-%s' % now)\n",
    "\n",
    "logdir = tb_logdir()\n",
    "print('Using %s for TensorBoard logs' % logdir)\n",
    "\n",
    "SAVED_MODEL_PATH = os.path.join(logdir, 'model.ckpt')\n",
    "\n",
    "'''\n",
    "# This could be useful later\n",
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "            tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "'''\n",
    "        \n",
    "with tf.Session() as sess:\n",
    "    file_writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            summary, _, _ = sess.run([summaries, training_op, extra_update_ops], feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: mnist.validation.images, y: mnist.validation.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Val accuracy\", acc_val)\n",
    "        file_writer.add_summary(summary, epoch)\n",
    "    save_path = saver.save(sess, SAVED_MODEL_PATH)\n",
    "    file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning\n",
    "Transfer learning refers to re-using a model that was trained for a different task, typically using only the lower, more generic layers. Below, the loading of TensorFlow models is illustrated with examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from mnist-logs/run-2018-09-04T09:52:21/model.ckpt\n",
      "Accuracy on test set: 0.9786\n"
     ]
    }
   ],
   "source": [
    "saved_model_meta = SAVED_MODEL_PATH + '.meta'\n",
    "saver = tf.train.import_meta_graph(SAVED_MODEL_PATH + '.meta')\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name('X:0')\n",
    "y = tf.get_default_graph().get_tensor_by_name('y:0')\n",
    "\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name('eval/accuracy/accuracy:0')\n",
    "# [n.name for n in tf.get_default_graph().as_graph_def().node]\n",
    "\n",
    "# for op in tf.get_default_graph().get_operations():\n",
    "#     print(op.name)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, SAVED_MODEL_PATH)\n",
    "    acc_test = accuracy.eval(feed_dict={X: mnist.validation.images, y: mnist.validation.labels})\n",
    "    \n",
    "print('Accuracy on test set:', acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8\n",
    "a. Build a neural network with five hidden layers of 100 neurons each, He initialization, and the ELU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/mnist/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/mnist/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/mnist/data/t10k-labels-idx1-ubyte.gz\n",
      "Using chap-11-exercise-8-logs/run-2018-09-04T09:52:51 for TensorBoard logs\n",
      "Epoch: 0 Training acc: 0.18981382 Validation acc: 0.6465154\n",
      "Epoch: 1 Training acc: 0.6465154 Validation acc: 0.50046366\n",
      "Epoch: 2 Training acc: 0.50046366 Validation acc: 0.574292\n",
      "Epoch: 3 Training acc: 0.574292 Validation acc: 0.48163208\n",
      "Epoch: 4 Training acc: 0.48163208 Validation acc: 0.5812112\n",
      "Epoch: 5 Training acc: 0.5812112 Validation acc: 0.7403524\n",
      "Epoch: 6 Training acc: 0.7403524 Validation acc: 0.7392111\n",
      "Epoch: 7 Training acc: 0.7392111 Validation acc: 0.862187\n",
      "Epoch: 8 Training acc: 0.862187 Validation acc: 0.9190385\n",
      "Epoch: 9 Training acc: 0.9190385 Validation acc: 0.92363936\n",
      "Epoch: 10 Training acc: 0.92363936 Validation acc: 0.90933734\n",
      "Epoch: 11 Training acc: 0.90933734 Validation acc: 0.9047721\n",
      "Epoch: 12 Training acc: 0.9047721 Validation acc: 0.91861045\n",
      "Epoch: 13 Training acc: 0.91861045 Validation acc: 0.91461587\n",
      "Epoch: 14 Training acc: 0.91461587 Validation acc: 0.92752695\n",
      "Epoch: 15 Training acc: 0.92752695 Validation acc: 0.93754905\n",
      "Epoch: 16 Training acc: 0.93754905 Validation acc: 0.94318426\n",
      "Epoch: 17 Training acc: 0.94318426 Validation acc: 0.94343394\n",
      "Epoch: 18 Training acc: 0.94343394 Validation acc: 0.9476068\n",
      "Epoch: 19 Training acc: 0.9476068 Validation acc: 0.95131606\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "N_INPUTS = 28*28\n",
    "N_NEURONS = 100\n",
    "N_LAYERS = 5\n",
    "N_CLASSES = 5\n",
    "N_EPOCHS = 20\n",
    "BATCH_SIZE = 50\n",
    "ROOT_LOGDIR = 'chap-11-exercise-8-logs'\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def build_placeholders(n_inputs):\n",
    "    X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "    y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def build_model(X, y, n_neurons, n_layers, n_classes, learning_rate):\n",
    "    \n",
    "    def build_hidden_layers(X, n_layers, n_neurons, init):\n",
    "        neuron_layer = partial(tf.layers.dense, activation=tf.nn.elu, kernel_initializer=init)\n",
    "        top_layer = X\n",
    "        for ind in range(1, n_layers + 1):\n",
    "            layer_name = 'hidden-layer-' + str(ind)\n",
    "            top_layer = neuron_layer(top_layer, n_neurons, name=layer_name)\n",
    "        return top_layer      \n",
    "\n",
    "    '''\n",
    "    def build_hidden_layers(X, n_layers, n_neurons, init):\n",
    "        neuron_layer = partial(tf.layers.dense, activation=tf.nn.elu, kernel_initializer=init)\n",
    "        hidden1 = neuron_layer(X, n_neurons, name='hidden1')\n",
    "        hidden2 = neuron_layer(hidden1, n_neurons, name='hidden2')\n",
    "        hidden3 = neuron_layer(hidden2, n_neurons, name='hidden3')\n",
    "        hidden4 = neuron_layer(hidden3, n_neurons, name='hidden4')\n",
    "        hidden5 = neuron_layer(hidden4, n_neurons, name='hidden5')\n",
    "        return hidden5\n",
    "    '''\n",
    "\n",
    "    with tf.name_scope(\"hidden\"):\n",
    "        he_init = tf.variance_scaling_initializer()\n",
    "        top_hidden = build_hidden_layers(X, n_layers=5, n_neurons=100, init=he_init)\n",
    "\n",
    "    \n",
    "    with tf.name_scope(\"logits\"):\n",
    "        logits = tf.layers.dense(top_hidden, n_classes, name=\"logits\")\n",
    "\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        # This op expects unscaled logits, since it performs a softmax on logits internally for efficiency.\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "        loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "    with tf.name_scope(\"train\"):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "    \n",
    "    # Evaluate performance by checking if the correct label is in top 1:\n",
    "    with tf.name_scope(\"eval\"):\n",
    "        correct = tf.nn.in_top_k(logits, y, 1)\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
    "        \n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    summaries = tf.summary.merge_all()\n",
    "    return { 'accuracy': accuracy, 'summaries': summaries, 'training_op': training_op }\n",
    "\n",
    "\n",
    "def tb_logdir(root_logdir):   \n",
    "    now = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "    return os.path.join(root_logdir, 'run-%s' % now)\n",
    "\n",
    "\n",
    "def init_feed_dict(mnist, X, y, n_classes):\n",
    "    \n",
    "    def extract_data(all_images, all_labels):\n",
    "        inds_used = mnist.train.labels < n_classes\n",
    "        images = mnist.train.images[inds_used]\n",
    "        labels = mnist.train.labels[inds_used]\n",
    "        return images, labels\n",
    "    \n",
    "    def feed_dict(train):\n",
    "        if train:\n",
    "            xs, ys = X_train, y_train\n",
    "        else:\n",
    "            xs, ys = X_val, y_val\n",
    "        return {X: xs, y: ys}\n",
    "    \n",
    "    train_images, train_labels = extract_data(mnist.train.images, mnist.train.labels)\n",
    "    val_images, val_labels = extract_data(mnist.validation.images, mnist.validation.labels)\n",
    "    \n",
    "    def feed_dict(train):\n",
    "        if train:\n",
    "            xs, ys = train_images, train_labels\n",
    "        else:\n",
    "            xs, ys = val_images, val_labels\n",
    "\n",
    "        return {X: xs, y: ys}\n",
    "    \n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "def main():\n",
    "    X, y = build_placeholders(n_inputs=N_INPUTS)\n",
    "    model = build_model(X=X, \n",
    "                        y=y, \n",
    "                        n_neurons=N_NEURONS, \n",
    "                        n_layers=N_LAYERS, \n",
    "                        n_classes=N_CLASSES,\n",
    "                        learning_rate=LEARNING_RATE)\n",
    "    accuracy, summaries, training_op = [model[key] for key in ['accuracy', 'summaries', 'training_op']]\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    mnist = input_data.read_data_sets(\"/tmp/mnist/data\")\n",
    "\n",
    "    logdir = tb_logdir(root_logdir=ROOT_LOGDIR)\n",
    "    print('Using %s for TensorBoard logs' % logdir)\n",
    "\n",
    "    saved_model_path = os.path.join(logdir, 'model.ckpt')\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        train_writer = tf.summary.FileWriter(logdir + '/train', sess.graph)\n",
    "        val_writer = tf.summary.FileWriter(logdir + '/val')\n",
    "        tf.global_variables_initializer().run()\n",
    "        feed_dict = init_feed_dict(mnist=mnist, X=X, y=y, n_classes=N_CLASSES)\n",
    "        for epoch in range(N_EPOCHS):\n",
    "            summary, train_acc, _ = sess.run([summaries, accuracy, training_op], feed_dict=feed_dict(train=True))\n",
    "            val_summary, val_acc = sess.run([summaries, accuracy], feed_dict=feed_dict(train=False))\n",
    "            train_writer.add_summary(summary, epoch)\n",
    "            val_writer.add_summary(val_summary, epoch)\n",
    "            print('Epoch:', epoch, 'Training acc:', train_acc, 'Validation acc:', val_acc)\n",
    "        train_writer.close()\n",
    "        val_writer.close()\n",
    "        \n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
