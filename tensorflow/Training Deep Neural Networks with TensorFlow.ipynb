{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training deep neural nets\n",
    "Adapted from Chap. 11 of `Hands-on Machine Learning with Scikit-Learn and TensorFlow` by A. Geron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main problems in training deep neural networks with millions of parametetrs\n",
    "1. Vanishing (or exploding) gradients that make lower layers hard to train\n",
    "2. Training is very slow\n",
    "3. Severe overfitting possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing gradients problem\n",
    "Backpropagation algorithm works by going from the output layer to the input layer, propagating the error gradient along the way. Gradients often get smaller and smaller as the algorithm progresses down to lower (closer to input) layers. Therefore, gradient descent (GD) update leaves the connection weights in the lower layers virtually unchanged and optimization never converges. Neural networks may also suffer from the _exploding gradients_ problem. This is especially the case with recurrent neural networks.\n",
    "\n",
    "In 2010, Glorot and Bengio showed that with logistic activation function and a simple mean-zero-std-one initialization, the variances of the outputs of each layer are much larger than the variances of the inputs. Therefore, the outputs in the top layers saturate close to zero or one and the gradients are very small. Therefore, the backpropagation algorithm has basically no gradient to propagate to the lower layers.\n",
    "\n",
    "As a solution, Glorot and Bengio proposed that the connection weights be initialized with zero-mean normal distribution with the following standard deviation $\\sigma$ or uniform distribution with the range $-r$ and $+r$:\n",
    "\n",
    "$$\n",
    "    \\sigma = \\sqrt{\\frac{2}{n_{inputs} + n_{outputs}}}, \\\\\n",
    "    r = \\sqrt{\\frac{6}{n_{inputs} + n_{outputs}}}\n",
    "$$\n",
    "Here $n_{inputs}$ and $n_{outputs}$ are the number of input and output connections for each layer. By default, `tf.layers.dense()` uses this initialization with a uniform distribution. One can use the similar _He initialization_ as follows:\n",
    "```python\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer(mode='FAN_AVG')\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, kernel_initializer=he_init, name=\"hidden1\")\n",
    "```\n",
    "\n",
    "### Nonsaturating activation functions\n",
    "As mentioned above, using the logistic activation function can lead to the vanishing gradients problem. For example, ReLU works well because it does not saturate for large positive values and it also fast to compute. ReLU suffers, however, partially from the same problem as the logistic activation function that the output values can saturate to zero. To solve this problem, one can use a variant of the ReLU such as `LeakyReLU`$(z)=\\max(\\alpha z, z)$, where typically $\\alpha=0.01$. The non-zero $alpha$ ensures that leaky ReLUs never completely die.\n",
    "\n",
    "Another variant is the exponential linear unit (ELU) defined as\n",
    "\n",
    "$$\n",
    "\\textrm{ELU}(z) = \\left\\{\\begin{array}{lr}\n",
    "        \\alpha(e^z - 1), & \\text{for } z < 0 \\\\\n",
    "        z, & \\text{for } z \\geq 0\n",
    "        \\end{array}\\right.\n",
    "$$\n",
    "\n",
    "Compared to ReLU, ELU has nonzero gradient for $z < 0$, is smooth everywhere, and has an average output closer to 0 at $x=0$. All these features mitigate the vanishing gradients problem. The only disadvantage with ELU is that it is slower to compute than ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch normalization\n",
    "He initialization and variants of ELU reduce the vanishing gradients problem at the beginning of training, but nothing guarantees it does not re-emerge during training. In 2015, Ioffe and Szegedy [proposed](https://arxiv.org/pdf/1502.03167v3.pdf) a technique called _Batch normalization_ (BN) to address the vanishing/exploding gradients problems, or more generally the problem that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change (Internal covariance shift problem). \n",
    "\n",
    "The technique consists of adding an operation in the model just before the activation function of each layer: zero-centering and normalizing the inputs, then scaling and shifting the result using two new parameters per layer (one for scaling, the other for shifting). This operationg lets the model learn the optimal scale and inputs for each layer.\n",
    "\n",
    "The algorithm estimates the inputs' mean and standard deviation by evaluating the mean and standard deviation of the inputs over the current mini-batch (hence the name Batch Normalization). In total, four parameters are learned for each batch-normalized layer: $\\gamma$ (scale of outputs), $\\beta$ (offset of outputs), $\\mu$ (mean of inputs), and $\\sigma$ (standard deviation of inputs).\n",
    "\n",
    "Ioffe and Szegedy showed that using batch normalization strongly reduced the vanishing gradients problem, reduced the sensitivity of training to the weight initialization, allowed for using much larger learning rates, and even acted as a regularization mitigating overfitting. Batch normalization adds complexity to the model and slows down training due to the extra computations required. Training can speed up once GD has found reasonably good values for the scales and offsets, though.\n",
    "\n",
    "See the example below for batch normalization in TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient clipping\n",
    "One technique to mitigate the exploding gradients problem is to [clip the gradients](http://proceedings.mlr.press/v28/pascanu13.pdf) during backpropagation so that they never exceed a given threshold. In TensorFlow, the optimizer's `minimize` function both computes the gradients and applies them to variables, so one must instead compute the gradients first, then create an operation to clip the gradients by value and finally apply the clipped gradients. The clip threshold is a hyperparameter that can be tuned. See the example below for gradient clipping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full example from Chap. 10 with He initialization, ELU activation function, batch normalization, and gradient clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from functools import partial\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "n_inputs = 28*28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\") # Number of training samples in batch not known and not required\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") # y is a 1D array with unknown length\n",
    "\n",
    "# This will be set to True during training to tell batch normalization layers to use the whole training set's mean and stddev\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer(mode='FAN_AVG')\n",
    "\n",
    "# BN uses exponential decay with momentum to compute the running averages\n",
    "batch_norm_layer = partial(tf.layers.batch_normalization, training=training, momentum=0.9)\n",
    "\n",
    "# Neural network with batch-normalized layers\n",
    "with tf.name_scope(\"ann\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\", kernel_initializer=he_init)\n",
    "    bn1 = batch_norm_layer(hidden1)\n",
    "    bn1_act = tf.nn.elu(bn1)\n",
    "    hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\", kernel_initializer=he_init)\n",
    "    bn2 = batch_norm_layer(hidden2)\n",
    "    bn2_act = tf.nn.elu(bn2)\n",
    "    logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\", kernel_initializer=he_init) # Unscaled as softmax computed later internally\n",
    "    logits = batch_norm_layer(logits_before_bn, training=training, momentum=0.9)\n",
    "    \n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    # This op expects unscaled logits, since it performs a softmax on logits internally for efficiency.\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "learning_rate = 0.05\n",
    "gradient_clip_threshold = 1.0\n",
    "\n",
    "# Training with gradient clipping\n",
    "with tf.name_scope(\"train\"):\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    grads_and_vars = optimizer.compute_gradients(loss)\n",
    "    capped_cvs = [(tf.clip_by_value(grad, -gradient_clip_threshold, gradient_clip_threshold), var) \n",
    "                  for grad, var in grads_and_vars]\n",
    "    training_op = optimizer.apply_gradients(capped_cvs, global_step=global_step)\n",
    "\n",
    "# Evaluate performance by checking if the correct label is in top 1:\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
    "\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "summaries = tf.summary.merge_all()\n",
    "   \n",
    "saver = tf.train.Saver()\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/mnist/data\")\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 50\n",
    "\n",
    "# Batch normalization creates operations that must be evaluated at each step to update the moving averages\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "root_logdir = 'mnist-logs'\n",
    "\n",
    "def tb_logdir():   \n",
    "    now = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "    return os.path.join(root_logdir, 'run-%s' % now)\n",
    "\n",
    "logdir = tb_logdir()\n",
    "print('Using %s for TensorBoard logs' % logdir)\n",
    "\n",
    "SAVED_MODEL_PATH = os.path.join(logdir, 'model.ckpt')\n",
    "\n",
    "'''\n",
    "# This could be useful later\n",
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "            tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "'''\n",
    "        \n",
    "with tf.Session() as sess:\n",
    "    file_writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            summary, _, _ = sess.run([summaries, training_op, extra_update_ops], feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: mnist.validation.images, y: mnist.validation.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Val accuracy\", acc_val)\n",
    "        file_writer.add_summary(summary, epoch)\n",
    "    save_path = saver.save(sess, SAVED_MODEL_PATH)\n",
    "    file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning\n",
    "Transfer learning refers to re-using a model that was trained for a different task, typically using only the lower, more generic layers. Below, the loading of TensorFlow models is illustrated with examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_meta = SAVED_MODEL_PATH + '.meta'\n",
    "saver = tf.train.import_meta_graph(SAVED_MODEL_PATH + '.meta')\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name('X:0')\n",
    "y = tf.get_default_graph().get_tensor_by_name('y:0')\n",
    "\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name('eval/accuracy/accuracy:0')\n",
    "# [n.name for n in tf.get_default_graph().as_graph_def().node]\n",
    "\n",
    "# for op in tf.get_default_graph().get_operations():\n",
    "#     print(op.name)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, SAVED_MODEL_PATH)\n",
    "    acc_test = accuracy.eval(feed_dict={X: mnist.validation.images, y: mnist.validation.labels})\n",
    "    \n",
    "print('Accuracy on test set:', acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8\n",
    "a. Build a neural network with five hidden layers of 100 neurons each, He initialization, and the ELU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/mnist/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/mnist/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/mnist/data/t10k-labels-idx1-ubyte.gz\n",
      "Using chap-11-exercise-8-logs/run-2018-09-03T17:24:59 for TensorBoard logs\n",
      "Epoch: 0 Training acc: 0.12942487 Test acc: 0.5911198\n",
      "Epoch: 1 Training acc: 0.5930005 Test acc: 0.48234665\n",
      "Epoch: 2 Training acc: 0.4835934 Test acc: 0.6235735\n",
      "Epoch: 3 Training acc: 0.628979 Test acc: 0.70435095\n",
      "Epoch: 4 Training acc: 0.7010254 Test acc: 0.85520685\n",
      "Epoch: 5 Training acc: 0.85831475 Test acc: 0.77193296\n",
      "Epoch: 6 Training acc: 0.7771734 Test acc: 0.8880171\n",
      "Epoch: 7 Training acc: 0.8912617 Test acc: 0.935271\n",
      "Epoch: 8 Training acc: 0.93397236 Test acc: 0.9292083\n",
      "Epoch: 9 Training acc: 0.9278199 Test acc: 0.9149429\n",
      "Epoch: 10 Training acc: 0.91391 Test acc: 0.9327746\n",
      "Epoch: 11 Training acc: 0.9301828 Test acc: 0.9468616\n",
      "Epoch: 12 Training acc: 0.9452519 Test acc: 0.94204706\n",
      "Epoch: 13 Training acc: 0.9419082 Test acc: 0.94347364\n",
      "Epoch: 14 Training acc: 0.9439144 Test acc: 0.95256776\n",
      "Epoch: 15 Training acc: 0.9526527 Test acc: 0.9497147\n",
      "Epoch: 16 Training acc: 0.95153815 Test acc: 0.9523894\n",
      "Epoch: 17 Training acc: 0.9529648 Test acc: 0.95773894\n",
      "Epoch: 18 Training acc: 0.9588497 Test acc: 0.9629101\n",
      "Epoch: 19 Training acc: 0.962996 Test acc: 0.964515\n",
      "Epoch: 20 Training acc: 0.9630852 Test acc: 0.96273184\n",
      "Epoch: 21 Training acc: 0.96232724 Test acc: 0.9671897\n",
      "Epoch: 22 Training acc: 0.965671 Test acc: 0.9657632\n",
      "Epoch: 23 Training acc: 0.9665626 Test acc: 0.96522826\n",
      "Epoch: 24 Training acc: 0.9666072 Test acc: 0.96380174\n",
      "Epoch: 25 Training acc: 0.9680785 Test acc: 0.9636234\n",
      "Epoch: 26 Training acc: 0.9679893 Test acc: 0.9636234\n",
      "Epoch: 27 Training acc: 0.9677218 Test acc: 0.9655849\n",
      "Epoch: 28 Training acc: 0.96834594 Test acc: 0.967903\n",
      "Epoch: 29 Training acc: 0.9692376 Test acc: 0.96861625\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "n_inputs = 28*28\n",
    "n_neurons = 100\n",
    "n_layers = 5\n",
    "n_classes = 5\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "he_init = tf.variance_scaling_initializer()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "'''\n",
    "def build_hidden_layers(X, n_layers, n_neurons, init):\n",
    "    neuron_layer = partial(tf.layers.dense, activation=tf.nn.elu, kernel_initializer=init)\n",
    "    top_layer = X\n",
    "    for ind in range(1, n_layers + 1):\n",
    "        layer_name = 'hidden-layer-' + str(ind)\n",
    "        top_layer = neuron_layer(top_layer, n_neurons, name=layer_name)\n",
    "    return top_layer\n",
    "'''        \n",
    "\n",
    "def build_hidden_layers(X, n_layers, n_neurons, init):\n",
    "    neuron_layer = partial(tf.layers.dense, activation=tf.nn.elu, kernel_initializer=init)\n",
    "    hidden1 = neuron_layer(X, n_neurons, name='hidden1')\n",
    "    hidden2 = neuron_layer(hidden1, n_neurons, name='hidden2')\n",
    "    hidden3 = neuron_layer(hidden2, n_neurons, name='hidden3')\n",
    "    hidden4 = neuron_layer(hidden3, n_neurons, name='hidden4')\n",
    "    hidden5 = neuron_layer(hidden4, n_neurons, name='hidden5')\n",
    "    return hidden5\n",
    "\n",
    "with tf.name_scope(\"hidden\"):\n",
    "    top_hidden = build_hidden_layers(X, n_layers=5, n_neurons=100, init=he_init)\n",
    "\n",
    "    \n",
    "with tf.name_scope(\"logits\"):\n",
    "    logits = tf.layers.dense(top_hidden, n_classes, name=\"logits\")\n",
    "\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    # This op expects unscaled logits, since it performs a softmax on logits internally for efficiency.\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "    \n",
    "# Evaluate performance by checking if the correct label is in top 1:\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
    "        \n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "summaries = tf.summary.merge_all()\n",
    "   \n",
    "saver = tf.train.Saver()\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/mnist/data\")\n",
    "\n",
    "n_epochs = 30\n",
    "batch_size = 50\n",
    "\n",
    "root_logdir = 'chap-11-exercise-8-logs'\n",
    "\n",
    "def tb_logdir():   \n",
    "    now = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "    return os.path.join(root_logdir, 'run-%s' % now)\n",
    "\n",
    "logdir = tb_logdir()\n",
    "print('Using %s for TensorBoard logs' % logdir)\n",
    "\n",
    "SAVED_MODEL_PATH = os.path.join(logdir, 'model.ckpt')\n",
    "\n",
    "inds_used = mnist.train.labels < 5\n",
    "images = mnist.train.images[inds_used]\n",
    "labels = mnist.train.labels[inds_used]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.20)\n",
    "\n",
    "def feed_dict(train):\n",
    "    if train:\n",
    "        xs, ys = X_train, y_train\n",
    "    else:\n",
    "        xs, ys = X_val, y_val\n",
    "    return {X: xs, y: ys}\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    train_writer = tf.summary.FileWriter(logdir + '/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(logdir + '/test')\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(n_epochs):\n",
    "        summary, train_acc, _ = sess.run([summaries, accuracy, training_op], feed_dict=feed_dict(train=True))\n",
    "        test_summary, test_acc = sess.run([summaries, accuracy], feed_dict=feed_dict(train=False))\n",
    "        train_writer.add_summary(summary, epoch)\n",
    "        test_writer.add_summary(test_summary, epoch)\n",
    "        print('Epoch:', epoch, 'Training acc:', train_acc, 'Test acc:', test_acc)\n",
    "    train_writer.close()\n",
    "    test_writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
