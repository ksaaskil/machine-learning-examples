{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders\n",
    "Based on Chap. 15 of `Hands-on Machine Learning with Scikit-Learn and TensorFlow` by A. GÃ©ron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoders are networks used for learning efficient representations of data, called _codings_. Autoencoders are used, e.g., for dimensionality reduction, feature extraction, and unsupervised pretraining. Generative autoencoders are able to create new data that look similar to the training data. Autoencoders are trained by expecting them to copy their inputs to outputs. By constraining the network to have, e.g., smaller size of codings than the input, the network cannot trivially copy the inputs to outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first create a simple linear autoencoder with a single layer, which can be shown to be equivalent to PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 4\n",
    "n_hidden = 2\n",
    "n_outputs = n_inputs\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs))\n",
    "# y = X\n",
    "\n",
    "codings = tf.layers.dense(X, n_hidden)\n",
    "outputs = tf.layers.dense(codings, n_outputs)\n",
    "\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(X - outputs))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(reconstruction_loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# iris dataset\n",
    "data = load_iris()\n",
    "X_train = data['data']\n",
    "X_train = StandardScaler().fit_transform(X=X_train)\n",
    "\n",
    "n_epochs = 1000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        mse, _ = sess.run([reconstruction_loss, training_op], feed_dict={X: X_train})\n",
    "        if epoch % 10 == 0:\n",
    "            print('Epoch: {}\\tMSE: {:.2f}'.format(epoch,mse))\n",
    "    projection = codings.eval(feed_dict={X: X_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_projection = pca.fit_transform(X=X_train)\n",
    "\n",
    "print('PCA explained variance ratio:', np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(projection[:, 1], projection[:, 0], label='NN')\n",
    "plt.scatter(pca_projection[:, 0], pca_projection[:, 1], c='r', label='PCA')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked autoencoders\n",
    "Autoencoder with multiple hidden layers is called a _stacked autoencoder_ (or deep autoencoder). Adding layers helps the network to learn more complex codings, but one must be careful not to make the encoder too powerful. The architecture of the stacked autoencoder is typically symmetrical with respect to the center (coding) layer. It is common to tie the weights of the decoder layers to be equal to the weights of the encoder layers. This reduces the risk of overfitting and makes training faster.\n",
    "\n",
    "There's no known easy way to tie the weights using `tf.layers.dense`, but it can be done by defining layers from scratch as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 28*28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 150 # codings\n",
    "n_hidden3 = n_hidden1\n",
    "n_outputs = n_inputs\n",
    "\n",
    "learning_rate = 0.01\n",
    "l2_reg = 0.0001\n",
    "\n",
    "activation = tf.nn.elu\n",
    "\n",
    "kernel_regularizer = tf.contrib.layers.l2_regularizer(l2_reg)\n",
    "kernel_initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs))\n",
    "\n",
    "weights1_init = kernel_initializer((n_inputs, n_hidden1))\n",
    "weights2_init = kernel_initializer((n_hidden1, n_hidden2))\n",
    "\n",
    "weights1 = tf.Variable(weights1_init, dtype=tf.float32, name='weights1')\n",
    "weights2 = tf.Variable(weights2_init, dtype=tf.float32, name='weights2')\n",
    "weights3 = tf.transpose(weights2, name='weights3')\n",
    "weights4 = tf.transpose(weights1, name='weights4')\n",
    "\n",
    "biases1 = tf.Variable(tf.zeros(n_hidden1), name='biases1')\n",
    "biases2 = tf.Variable(tf.zeros(n_hidden2), name='biases2')\n",
    "biases3 = tf.Variable(tf.zeros(n_hidden3), name='biases3')\n",
    "biases4 = tf.Variable(tf.zeros(n_outputs), name='biases4')\n",
    "\n",
    "hidden1 = activation(tf.matmul(X, weights1) + biases1)\n",
    "hidden2 = activation(tf.matmul(hidden1, weights2) + biases2)\n",
    "codings = hidden2\n",
    "hidden3 = activation(tf.matmul(hidden2, weights3) + biases3)\n",
    "outputs = activation(tf.matmul(hidden3, weights4) + biases4)\n",
    "\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))\n",
    "reg_loss = kernel_regularizer(weights1) + kernel_regularizer(weights2)\n",
    "\n",
    "loss = reconstruction_loss + reg_loss\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/mnist/data\")\n",
    "\n",
    "X_train = mnist.train.images\n",
    "\n",
    "X_validation = mnist.validation.images\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        mse, _ = sess.run([loss, training_op], feed_dict={X: X_train})\n",
    "        if epoch % 10 == 0:\n",
    "            print('Epoch: {}\\tMSE: {:.2f}'.format(epoch,mse))\n",
    "    outputs_val = outputs.eval(feed_dict={X: X_validation})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(img, shape=(28, 28)):\n",
    "    img = img.reshape(shape)\n",
    "    plt.imshow(img, cmap='Greys', interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    \n",
    "n_test_digits = 10\n",
    "\n",
    "for i in range(n_test_digits):\n",
    "    plt.subplot(n_test_digits, 2, i * 2 + 1)\n",
    "    plot_image(X_validation[i])\n",
    "    plt.subplot(n_test_digits, 2, i * 2 + 2)\n",
    "    plot_image(outputs_val[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of training the stacked autoencoder in one shot, it's often better to train shallow autoencoders one-by-one and then stack them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0ab915247b13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mreset_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mweights1_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_hidden1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_inputs' is not defined"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "\n",
    "weights1_init = kernel_initializer((n_inputs, n_hidden1))\n",
    "weights2_init = kernel_initializer((n_hidden1, n_hidden2))\n",
    "\n",
    "weights1 = tf.Variable(weights1_init, dtype=tf.float32, name='weights1')\n",
    "weights2 = tf.Variable(weights2_init, dtype=tf.float32, name='weights2')\n",
    "weights3 = tf.transpose(weights2, name='weights3')\n",
    "weights4 = tf.transpose(weights1, name='weights4')\n",
    "\n",
    "biases1 = tf.Variable(tf.zeros(n_hidden1), name='biases1')\n",
    "biases2 = tf.Variable(tf.zeros(n_hidden2), name='biases2')\n",
    "biases3 = tf.Variable(tf.zeros(n_hidden3), name='biases3')\n",
    "biases4 = tf.Variable(tf.zeros(n_outputs), name='biases4')\n",
    "\n",
    "hidden1 = activation(tf.matmul(X, weights1) + biases1)\n",
    "hidden2 = activation(tf.matmul(hidden1, weights2) + biases2, name='codings')\n",
    "\n",
    "hidden3 = activation(tf.matmul(hidden2, weights3) + biases3)\n",
    "outputs = activation(tf.matmul(hidden3, weights4) + biases4)\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name='optimizer')\n",
    "\n",
    "with tf.name_scope('phase1'):\n",
    "    phase1_outputs = tf.matmul(hidden1, weights4) + biases4\n",
    "    phase1_reconstruction_loss = tf.reduce_mean(tf.square(phase1_outputs - X))\n",
    "    phase1_reg_loss = regularizer(weights1)\n",
    "    phase1_loss = phase1_reconstruction_loss + phase1_reg_loss\n",
    "    phase1_training_op = optimizer.minimize(phase1_loss)\n",
    "    \n",
    "with tf.name_scope('phase2'):\n",
    "    phase2_reconstruction_loss = tf.reduce_mean(tf.square(hidden3 - hidden1))\n",
    "    phase2_reg_loss = regularizer(weights2)\n",
    "    phase2_loss = phase2_reconstruction_loss + phase2_reg_loss\n",
    "    train_vars = [weights2, biases2]\n",
    "    phase2_training_op = optimizer.minimize(phase2_loss, var_list=train_vars)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/mnist/data\")\n",
    "\n",
    "X_train = mnist.train.images\n",
    "\n",
    "X_validation = mnist.validation.images\n",
    "\n",
    "n_epochs = 10 # Per phase\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # Phase 1\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        mse, _ = sess.run([phase1_loss, phase1_training_op], feed_dict={X: X_train})\n",
    "        if epoch % 10 == 0:\n",
    "            print('Phase 1 epoch: {}\\tMSE: {:.2f}'.format(epoch,mse))\n",
    "    h1_cache = hidden1.eval(feed_dict={X: X_train}) # Evaluate first layer outputs and feed in next phase\n",
    "    phase1_outputs_val = phase1_outputs.eval(feed_dict={X: X_validation})\n",
    "    \n",
    "    # Phase 2\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        mse, _ = sess.run([phase2_loss, phase2_training_op], feed_dict={hidden1: h1_cache})\n",
    "        if epoch % 10 == 0:\n",
    "            print('Phase 2 epoch: {}\\tMSE: {:.2f}'.format(epoch,mse))\n",
    "    phase2_outputs_val = outputs.eval(feed_dict={X: X_validation})\n",
    "    \n",
    "    saver.save(sess, './tf-logs/chap-15-stacked-autoencoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(img, shape=(28, 28)):\n",
    "    img = img.reshape(shape)\n",
    "    plt.imshow(img, cmap='Greys', interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    \n",
    "n_test_digits = 10\n",
    "\n",
    "for i in range(n_test_digits):\n",
    "    plt.subplot(n_test_digits, 3, i * 3 + 1)\n",
    "    plot_image(X_validation[i])\n",
    "    plt.subplot(n_test_digits, 3, i * 3 + 2)\n",
    "    plot_image(phase1_outputs_val[i])\n",
    "    plt.subplot(n_test_digits, 3, i * 3 + 3)\n",
    "    plot_image(phase2_outputs_val[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised pretraining\n",
    "It is typical in supervised machine learning that there are lots more training instances without labels available than instances without labels. For example, there are many more images available in the net than images with labels. In such cases, it makes sense to pretrain the network in unsupervised fashion: training an autoencoder to extract the relevant features, and then use the labeled data to train the classifier with the labeled data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/mnist/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/mnist/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/mnist/data/t10k-labels-idx1-ubyte.gz\n",
      "INFO:tensorflow:Restoring parameters from ./tf-logs/chap-15-stacked-autoencoder\n",
      "Classifier epoch: 50\tTrain loss: 0.31\tTrain acc: 90.611 %\tVal loss: 0.30\tVal acc: 90.64 %\n",
      "Classifier epoch: 100\tTrain loss: 0.21\tTrain acc: 93.755 %\tVal loss: 0.20\tVal acc: 93.74 %\n",
      "Classifier epoch: 150\tTrain loss: 0.16\tTrain acc: 95.260 %\tVal loss: 0.16\tVal acc: 95.44 %\n",
      "Classifier epoch: 200\tTrain loss: 0.12\tTrain acc: 96.333 %\tVal loss: 0.14\tVal acc: 96.16 %\n",
      "Classifier epoch: 250\tTrain loss: 0.10\tTrain acc: 97.035 %\tVal loss: 0.12\tVal acc: 96.58 %\n",
      "Classifier epoch: 300\tTrain loss: 0.08\tTrain acc: 97.576 %\tVal loss: 0.11\tVal acc: 96.78 %\n",
      "Classifier epoch: 350\tTrain loss: 0.07\tTrain acc: 98.005 %\tVal loss: 0.10\tVal acc: 97.22 %\n",
      "Classifier epoch: 400\tTrain loss: 0.06\tTrain acc: 98.395 %\tVal loss: 0.10\tVal acc: 97.32 %\n",
      "Classifier epoch: 450\tTrain loss: 0.39\tTrain acc: 91.636 %\tVal loss: 0.19\tVal acc: 94.94 %\n"
     ]
    }
   ],
   "source": [
    "# Dense classifier on top of the coding layer\n",
    "reset_graph()\n",
    "saver = tf.train.import_meta_graph('./tf-logs/chap-15-stacked-autoencoder.meta')\n",
    "\n",
    "n_hidden1 = 100\n",
    "n_hidden2 = 50\n",
    "n_classes = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "X = graph.get_tensor_by_name('X:0')\n",
    "y = tf.placeholder(tf.int32, shape=(None))\n",
    "codings = graph.get_tensor_by_name('codings:0')\n",
    "\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer(mode='FAN_AVG')\n",
    "dense_layer = partial(tf.layers.dense, activation=tf.nn.elu, kernel_initializer=he_init)\n",
    "\n",
    "with tf.name_scope('classifier'):\n",
    "    fc_layer1 = dense_layer(codings, n_hidden1, name='fc_layer1')\n",
    "    fc_layer2 = dense_layer(fc_layer1, n_hidden2, name='fc_layer2')\n",
    "    logits = dense_layer(fc_layer2, n_classes, activation=None, name='logits')\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')\n",
    "\n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
    "    \n",
    "with tf.name_scope(\"train_clf\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                   scope=\"logits|fc_layer[12]\") # Only train the classifier layer\n",
    "    assert len(train_vars) == 6 # Three layers with weights and biases each\n",
    "    training_op = optimizer.minimize(loss, var_list=train_vars)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/mnist/data\")\n",
    "\n",
    "X_train = mnist.train.images\n",
    "y_train = mnist.train.labels\n",
    "\n",
    "X_validation = mnist.validation.images\n",
    "y_validation = mnist.validation.labels\n",
    "\n",
    "n_epochs = 500\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run() # Initialize variables (required for those not in the restored graph)\n",
    "    saver.restore(sess, './tf-logs/chap-15-stacked-autoencoder') # Restore autoencoder\n",
    "    codings_train = codings.eval(feed_dict={X: X_train})\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_loss, train_acc, _ = sess.run([loss, accuracy, training_op], feed_dict={codings: codings_train, y: y_train})\n",
    "        if epoch % 50 == 0:\n",
    "            val_loss, val_acc = sess.run([loss, accuracy], feed_dict={X: X_validation, y:y_validation})\n",
    "            print('Classifier epoch: {}\\tTrain loss: {:.2f}\\tTrain acc: {:.3f} %\\tVal loss: {:.2f}\\tVal acc: {:.2f} %'\n",
    "                  .format(epoch, train_loss, train_acc*100, val_loss, val_acc * 100))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
