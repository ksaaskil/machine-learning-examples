{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Train NER with SpaCy\n",
        "\n",
        "See [train_ner.py](https://github.com/explosion/spaCy/blob/master/examples/training/train_ner.py)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import spacy\n",
        "from spacy.util import minibatch, compounding\n",
        "import random\n",
        "\n",
        "# training data\n",
        "TRAIN_DATA = [\n",
        "    (\"Who is Shaka Khan?\", {\"entities\": [(7, 17, \"PERSON\")]}),\n",
        "    (\"I like London and Berlin.\", {\"entities\": [(7, 13, \"LOC\"), (18, 24, \"LOC\")]}),\n",
        "]"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_nlp(model=None):\n",
        "  if model is not None:\n",
        "        nlp = spacy.load(model)  # load existing spaCy model\n",
        "        print(\"Loaded model '%s'\" % model)\n",
        "  else:\n",
        "      nlp = spacy.blank(\"en\")  # create blank Language class\n",
        "      print(\"Created blank 'en' model\")\n",
        "  return nlp\n",
        "\n",
        "def train(model=None, output_dir=None, n_iter=10):\n",
        "    \"\"\"Load the model, set up the pipeline and train the entity            recognizer.\"\"\"\n",
        "    nlp = get_nlp(model)\n",
        "\n",
        "    # create the built-in pipeline components and add them to the pipeline\n",
        "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
        "    if \"ner\" not in nlp.pipe_names:\n",
        "        ner = nlp.create_pipe(\"ner\")\n",
        "        nlp.add_pipe(ner, last=True)\n",
        "    # otherwise, get it so we can add labels\n",
        "    else:\n",
        "        ner = nlp.get_pipe(\"ner\")\n",
        "        \n",
        "    def add_labels(ner):\n",
        "        # add labels\n",
        "        for _, annotations in TRAIN_DATA:\n",
        "            for ent in annotations.get(\"entities\"):\n",
        "                ner.add_label(ent[2])\n",
        "\n",
        "    add_labels(ner)\n",
        "\n",
        "    # get names of other pipes to disable them during training\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
        "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "        # reset and initialize the weights randomly â€“ but only if we're\n",
        "        # training a new model\n",
        "        if model is None:\n",
        "            nlp.begin_training()\n",
        "        for itn in range(n_iter):\n",
        "            random.shuffle(TRAIN_DATA)\n",
        "            losses = {}\n",
        "            # batch up the examples using spaCy's minibatch\n",
        "            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
        "            for batch in batches:\n",
        "                texts, annotations = zip(*batch)\n",
        "                nlp.update(\n",
        "                    texts,  # batch of texts\n",
        "                    annotations,  # batch of annotations\n",
        "                    drop=0.5,  # dropout - make it harder to memorise data\n",
        "                    losses=losses,\n",
        "                )\n",
        "            print(\"Losses\", losses)\n",
        "    return nlp\n",
        "\n",
        "def test(nlp):\n",
        "    # test the trained model\n",
        "    for text, _ in TRAIN_DATA:\n",
        "        doc = nlp(text)\n",
        "        print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
        "        print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
        "\n",
        "def save(nlp, output_dir=None):\n",
        "      output_dir = Path(output_dir)\n",
        "      if not output_dir.exists():\n",
        "          output_dir.mkdir()\n",
        "      nlp.to_disk(output_dir)\n",
        "      print(\"Saved model to\", output_dir)\n",
        "        \n",
        "def load(output_dir):\n",
        "      # test the saved model\n",
        "      print(\"Loading from\", output_dir)\n",
        "      nlp = spacy.load(output_dir)\n",
        "      for text, _ in TRAIN_DATA:\n",
        "          doc = nlp(text)\n",
        "          print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
        "          print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
        "\n",
        "\n",
        "def main():\n",
        "  nlp = train()\n",
        "  test(nlp)\n",
        "  save(nlp, output_dir=\"models\")\n",
        "  nlp2 = load(output_dir=\"models\")\n",
        "  # Expected output:\n",
        "  # Entities [('Shaka Khan', 'PERSON')]\n",
        "  # Tokens [('Who', '', 2), ('is', '', 2), ('Shaka', 'PERSON', 3),\n",
        "  # ('Khan', 'PERSON', 1), ('?', '', 2)]\n",
        "  # Entities [('London', 'LOC'), ('Berlin', 'LOC')]\n",
        "  # Tokens [('I', '', 2), ('like', '', 2), ('London', 'LOC', 3),\n",
        "  # ('and', '', 2), ('Berlin', 'LOC', 3), ('.', '', 2)]"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created blank 'en' model\n",
            "Losses {'ner': 9.777777671813965}\n",
            "Losses {'ner': 9.541139602661133}\n",
            "Losses {'ner': 9.168060302734375}\n",
            "Losses {'ner': 8.870099544525146}\n",
            "Losses {'ner': 8.51644766330719}\n",
            "Losses {'ner': 8.410200238227844}\n",
            "Losses {'ner': 7.853942275047302}\n",
            "Losses {'ner': 7.871019959449768}\n",
            "Losses {'ner': 7.178551375865936}\n",
            "Losses {'ner': 6.534702003002167}\n",
            "Entities []\n",
            "Tokens [('Who', '', 2), ('is', '', 2), ('Shaka', '', 2), ('Khan', '', 2), ('?', '', 2)]\n",
            "Entities []\n",
            "Tokens [('I', '', 2), ('like', '', 2), ('London', '', 2), ('and', '', 2), ('Berlin', '', 2), ('.', '', 2)]\n",
            "Saved model to models\n",
            "Loading from models\n",
            "Entities []\n",
            "Tokens [('Who', '', 2), ('is', '', 2), ('Shaka', '', 2), ('Khan', '', 2), ('?', '', 2)]\n",
            "Entities []\n",
            "Tokens [('I', '', 2), ('like', '', 2), ('London', '', 2), ('and', '', 2), ('Berlin', '', 2), ('.', '', 2)]\n"
          ]
        }
      ],
      "execution_count": 8,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "ner-3.6.2"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.2",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "ner-3.6.2",
      "language": "python",
      "display_name": "Python ner-3.6.2"
    },
    "nteract": {
      "version": "0.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}